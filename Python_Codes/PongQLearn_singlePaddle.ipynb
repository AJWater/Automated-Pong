{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "from numpy import convolve\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 800\n",
    "HEIGHT = 600\n",
    "maxScore = 1\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.init()\n",
    "pygame.mixer.init()\n",
    "font = pygame.font.Font(None, 30)\n",
    "bigFont = pygame.font.Font(None, 72)\n",
    "pygame.display.set_caption(\"Pong Game\")\n",
    "pygame.mouse.set_visible(0)\n",
    "clock = pygame.time.Clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRunningAverage(totalRewards):\n",
    "    N = len(totalRewards)\n",
    "    window = 100\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(totalRewards[max(0, t-window):(t+1)])\n",
    "    plt.plot(running_avg)\n",
    "    #plt.title()\n",
    "    plt.show()\n",
    "    \n",
    "def movingaverage(values, window):\n",
    "    weights = np.repeat(1.0, window)/window\n",
    "    sma = np.convolve(values, weights, 'valid')\n",
    "    return sma    \n",
    "    \n",
    "    \n",
    "class Pong_Env():\n",
    "    \n",
    "#     self.score1 = 0\n",
    "#     self.score2 = 0\n",
    "#     self.player1 = 0\n",
    "#     self.player2 = 0\n",
    "#     self.time = 0\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.time = 0\n",
    "        self.L_paddle_xLoc = 50\n",
    "        self.R_paddle_xLoc = WIDTH - 50\n",
    "        self.LeftPaddle_center = np.array([self.L_paddle_xLoc, (HEIGHT/2)])\n",
    "        self.RightPaddle_center = np.array([self.R_paddle_xLoc, (HEIGHT/2)])\n",
    "        self.Paddle_thickness = 5\n",
    "        self.Paddle_length = 45\n",
    "        self.paddle_ymin = 50\n",
    "        self.paddle_ymax = (HEIGHT - 50)\n",
    "        \n",
    "        # [    0,         1,          2,          3    ]\n",
    "        # [left side, right side, top side, bottom side]\n",
    "        self.LeftPaddle_area = np.array([(self.LeftPaddle_center[0] - self.Paddle_thickness), (self.LeftPaddle_center[0] + self.Paddle_thickness),\n",
    "                              (self.LeftPaddle_center[1] - self.Paddle_length), (self.LeftPaddle_center[1] + self.Paddle_length)])\n",
    "        self.RightPaddle_area = np.array([(self.RightPaddle_center[0] - self.Paddle_thickness), (self.RightPaddle_center[0] + self.Paddle_thickness),\n",
    "                              (self.RightPaddle_center[1] - self.Paddle_length), (self.RightPaddle_center[1] + self.Paddle_length)])\n",
    "        self.Paddle_vel = 5\n",
    "        self.LeftPaddle_speed = 0\n",
    "        self.RightPaddle_speed = 0\n",
    "        self.Ball_center = np.array([(WIDTH/2), (HEIGHT/2)])\n",
    "        self.Ball_radius = 5\n",
    "        self.x_vel = 0\n",
    "        self.y_vel = 0\n",
    "        self.velocity = np.array([0.0, 0.0])\n",
    "        self.Ball_speed = 8.0\n",
    "        self.MaxSpeed = 25\n",
    "        self.state = None\n",
    "#        self.next_state = None\n",
    "        self.steps_beyond_done = None\n",
    "        self.R_action_space = ['up', 'still', 'down']\n",
    "        self.L_action_space = ['up', 'down']\n",
    "        self.L_state_space = ['left_paddle_y', 'ball_x', 'ball_y', 'ball_speed', 'ball_xvel', 'ball_yvel']\n",
    "        self.R_state_space = ['right_paddle_y', 'ball_x', 'ball_y', 'ball_speed', 'ball_xvel', 'ball_yvel']\n",
    "        self.L_reward = 0\n",
    "        self.R_reward = 0\n",
    "        self.L_bounce_tally = 0\n",
    "        self.R_bounce_tally = 0\n",
    "        self.L_score = 0\n",
    "        self.R_score = 0\n",
    "        self.done = False\n",
    "        #self.reset()\n",
    "    \n",
    "    \n",
    "    def OutOfBoundsMove(self, new_State_PadY):\n",
    "        self.new_State_PadY = new_State_PadY\n",
    "        if self.new_State_PadY < self.paddle_ymin or \\\n",
    "            self.new_State_PadY > self.paddle_ymax:\n",
    "#            print('first')\n",
    "            return True\n",
    "        \n",
    "#         elif self.new_state[1] < self.paddle_ymin or \\\n",
    "#             self.new_state[1] > self.paddle_ymax:\n",
    "# #            print('second')\n",
    "#             return True\n",
    "        \n",
    "        else:\n",
    "#            print('second')\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def step(self, L_action, R_action):\n",
    "#        if self.time >= 50:              # provides a short delay before the ball starts to move\n",
    "   #     print('testStepState =', state)\n",
    "   #     state = self.state\n",
    "     #   next_state = self.next_state\n",
    "        L_state = [self.LeftPaddle_center[1], self.Ball_center[0], self.Ball_center[1], \\\n",
    "            self.Ball_speed, self.velocity[0], self.velocity[1]]\n",
    "        R_state = [self.RightPaddle_center[1], self.Ball_center[0], self.Ball_center[1], \\\n",
    "            self.Ball_speed, self.velocity[0], self.velocity[1]]\n",
    "   #     print('stateStepBeginning=', state)\n",
    "        LeftPaddle_startPos = self.LeftPaddle_center[1]\n",
    "        RightPaddle_startPos = self.RightPaddle_center[1]\n",
    "        if L_action == 0:\n",
    "            self.LeftPaddle_speed = -1 * self.Paddle_vel # Move UP\n",
    "#             elif L_action == 1:\n",
    "#                 self.LeftPaddle_speed = 0 # Stay Still\n",
    "        else: # L_action = 1\n",
    "            self.LeftPaddle_speed = self.Paddle_vel # Move Down\n",
    "\n",
    "\n",
    "        if R_action == 0:\n",
    "            self.RightPaddle_speed = -1 * self.Paddle_vel # Move UP\n",
    "        elif R_action == 1:\n",
    "            self.RightPaddle_speed = 0 # Stay Still\n",
    "        else: # R_action = 2\n",
    "            self.RightPaddle_speed = self.Paddle_vel # Move Down\n",
    "\n",
    "\n",
    "        #self.RightPaddle_KeyPress()  # Checks for if user wants to move right paddle with arrow keys\n",
    "        \n",
    "    #    print('before =', self.velocity)\n",
    "\n",
    "        self.Ball_center += (self.Ball_speed * self.velocity) #* tau # Moves ball\n",
    "        #print(self.Ball_center, self.LeftPaddle_center[1], self.RightPaddle_center[1])\n",
    "\n",
    "\n",
    "        self.LeftPaddle_center[1] += (self.LeftPaddle_speed) #* tau # Moves left paddle\n",
    "        self.LeftPaddle_area = np.array([(self.LeftPaddle_center[0] - self.Paddle_thickness), (self.LeftPaddle_center[0] + self.Paddle_thickness),\n",
    "                          (self.LeftPaddle_center[1] - self.Paddle_length), (self.LeftPaddle_center[1] + self.Paddle_length)])\n",
    "            \n",
    "\n",
    "        self.RightPaddle_center[1] += (self.RightPaddle_speed) #* tau # Moves right paddle\n",
    "        self.RightPaddle_area = np.array([(self.RightPaddle_center[0] - self.Paddle_thickness), (self.RightPaddle_center[0] + self.Paddle_thickness),\n",
    "                          (self.RightPaddle_center[1] - self.Paddle_length), (self.RightPaddle_center[1] + self.Paddle_length)])\n",
    "\n",
    "        \n",
    "        \n",
    "        self.Paddle_Bounce() # test for paddle bounces\n",
    "        self.TandB_Bounce() # test for top and bottom wall bounces\n",
    "        self.SideWall_Bounce() # test for side wall bounces\n",
    "\n",
    "\n",
    "        # Determines the new state\n",
    "        next_L_state = self.LeftPaddle_center[1], self.Ball_center[0],\\\n",
    "                    self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1]\n",
    "        next_R_state = self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "                    self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1]\n",
    "   #     print('next_state =', next_state)\n",
    "    \n",
    "        LeftNoMove_next_state = LeftPaddle_startPos, self.Ball_center[0],\\\n",
    "                    self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1]\n",
    "        \n",
    "        RightNoMove_next_state = RightPaddle_startPos, self.Ball_center[0],\\\n",
    "                    self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1]\n",
    "        \n",
    "        \n",
    "        Leftwallmove = self.OutOfBoundsMove(next_L_state[0])\n",
    "        Rightwallmove = self.OutOfBoundsMove(next_R_state[0])\n",
    "   #     print('NoMove_next_state =', NoMove_next_state)\n",
    "   #     print()\n",
    "   #     self.next_state = self.LeftPaddle_center[1], self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "   #                 self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1]\n",
    "        self.done = self.Ball_center[0] < -5 or \\\n",
    "            self.Ball_center[0] > WIDTH + 5\n",
    "        self.done = bool(self.done)\n",
    "        \n",
    "        if not self.done:\n",
    "       #     print('in loop =', self.velocity)\n",
    "             if self.Ball_center[0] >= (self.LeftPaddle_area[0] - self.Ball_radius) and \\\n",
    "                self.Ball_center[0] <= (self.LeftPaddle_area[1] + 2.5 * self.Ball_radius) and \\\n",
    "                self.Ball_center[1] >= (self.LeftPaddle_area[2] - 2 * self.Ball_radius) and \\\n",
    "                self.Ball_center[1] <= (self.LeftPaddle_area[3] + 2 * self.Ball_radius) and \\\n",
    "                self.velocity[0] <= 0 and \\\n",
    "                self.Ball_center[0] >= 50 and \\\n",
    "                self.Ball_center[1] >= 140 and \\\n",
    "                self.Ball_center[1] <= (HEIGHT - 140):   #Ball hits Left Paddle when it was heading towards Left goal\n",
    "                    self.L_bounce_tally += 1\n",
    "                 #   print('paddle_bounce')\n",
    "                    self.L_reward = 50.0 #* self.L_bounce_tally\n",
    "                 #   print('L_bounce')\n",
    "                    self.R_reward = .1\n",
    "                    #print('Lr =', L_reward)\n",
    "                    #print('Rr =', R_reward)\n",
    "                    #print('activate1')\n",
    "                    #self.L_score += self.L_reward\n",
    "                    #self.R_score += self.R_reward\n",
    "                    self.time += 1\n",
    "                    \n",
    "                    if Leftwallmove == True:\n",
    "    #                    print('me2')\n",
    "                        self.LeftPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = LeftNoMove_next_state\n",
    "                        next_L_state = LeftNoMove_next_state\n",
    "                        self.L_reward = -2\n",
    "                   \n",
    "                        self.L_score += self.L_reward\n",
    "                    else:\n",
    "                        self.L_score += self.L_reward\n",
    "                    \n",
    "                  \n",
    "                    if Rightwallmove == True:\n",
    "    #                    print('me3')\n",
    "                        self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = RightNoMove_next_state\n",
    "                        next_R_state = RightNoMove_next_state\n",
    "\n",
    "                        self.R_reward = -1\n",
    "\n",
    "                        self.R_score += self.R_reward\n",
    "                    else:\n",
    "                        self.R_score += self.R_reward\n",
    "                  \n",
    "                    return next_L_state, next_R_state, self.L_reward, self.R_reward, self.done, {}\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Ball hits Right Paddle when it was heading towards Right goal\n",
    "    \n",
    "\n",
    "             if self.Ball_center[0] >= (self.RightPaddle_area[0] - 3.0 * self.Ball_radius) and \\\n",
    "                 self.Ball_center[0] <= (self.RightPaddle_area[1] + self.Ball_radius) and \\\n",
    "                 self.Ball_center[1] >= (self.RightPaddle_area[2] - 2.5 * self.Ball_radius) and \\\n",
    "                 self.Ball_center[1] <= (self.RightPaddle_area[3] + 2.5 * self.Ball_radius) and \\\n",
    "                 self.velocity[0] >= 0 and \\\n",
    "                 self.Ball_center[0] <= (WIDTH - 50) and \\\n",
    "                 self.Ball_center[1] >= 140 and \\\n",
    "                 self.Ball_center[1] <= (HEIGHT - 140):\n",
    "                    self.R_bounce_tally += 1\n",
    "                    self.R_reward = 50.0 #* self.R_bounce_tally\n",
    "                #    print('R_bounce')\n",
    "                    self.L_reward = .1\n",
    "                   # print('Lr =', L_reward)\n",
    "                   # print('Rr =', R_reward)\n",
    "                   # print('activate2')\n",
    "                    self.L_score += self.L_reward\n",
    "                    self.R_score += self.R_reward\n",
    "                    self.time += 1\n",
    "                    if Leftwallmove == True:\n",
    "                       # print('me2')\n",
    "                        self.LeftPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = LeftNoMove_next_state\n",
    "                        next_L_state = LeftNoMove_next_state\n",
    "                        self.L_reward = -2\n",
    "                        self.L_score += self.L_reward\n",
    "                    else:\n",
    "                        self.L_score += self.L_reward\n",
    "                        \n",
    "                        \n",
    "                    if Rightwallmove == True:\n",
    "    #                    print('me3')\n",
    "                        self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = RightNoMove_next_state\n",
    "                        next_R_state = RightNoMove_next_state  \n",
    "                        self.R_reward = -2\n",
    "                        self.R_score += self.R_reward\n",
    "                    else:\n",
    "                        self.R_score += self.R_reward \n",
    "                  \n",
    "                    return next_L_state, next_R_state, self.L_reward, self.R_reward, self.done, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 elif self.velocity[0] > 0 and \\\n",
    "#                     self.state[1] in range(160, (HEIGHT - 160))\n",
    "\n",
    "#             elif self.Ball_center[0] >= 0 and \\\n",
    "#                 self.Ball_center[0] <= 50 and \\\n",
    "#                 self.Ball_center[1] >= 140 and \\\n",
    "#                 self.Ball_center[1] <= (HEIGHT - 140):\n",
    "#                     self.L_reward = -1\n",
    "#                     self.R_reward = 1\n",
    "#                     self.L_score += self.L_reward\n",
    "#                     self.R_score += self.R_reward\n",
    "#                     self.time += 1\n",
    "#                     if not self.OutOfBoundsMove(next_state):\n",
    "#       #                  print('me5')\n",
    "#                         return next_state, self.L_reward, self.R_reward, self.done, {}\n",
    "#                     else:\n",
    "#     #                    print('me6')\n",
    "#                         self.LeftPaddle_center[1], self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "#                             self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = NoMove_next_state\n",
    "#                         self.L_reward = -1\n",
    "#                         self.R_reward = -1\n",
    "#                         self.L_score += self.L_reward\n",
    "#                         self.R_score += self.R_reward\n",
    "#                         return NoMove_next_state, self.L_reward, self.R_reward, self.done, {}\n",
    "\n",
    "#             elif self.Ball_center[0] >= (WIDTH - 50) and \\\n",
    "#                 self.Ball_center[0] <= WIDTH and \\\n",
    "#                 self.Ball_center[1] >= 140 and \\\n",
    "#                 self.Ball_center[1] <= (HEIGHT - 140):\n",
    "#                     self.R_reward = -1\n",
    "#                     self.L_reward = 1\n",
    "#                     self.L_score += self.L_reward\n",
    "#                     self.R_score += self.R_reward\n",
    "#                     self.time += 1\n",
    "#                     if not self.OutOfBoundsMove(next_state):\n",
    "#       #                  print('me7')\n",
    "#                         return next_state, self.L_reward, self.R_reward, self.done, {}\n",
    "#                     else:\n",
    "#      #                   print('me8')\n",
    "#                         self.LeftPaddle_center[1], self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "#                             self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = NoMove_next_state\n",
    "#                         self.L_reward = -1\n",
    "#                         self.R_reward = -1\n",
    "#                         self.L_score += self.L_reward\n",
    "#                         self.R_score += self.R_reward\n",
    "#                         return NoMove_next_state, self.L_reward, self.R_reward, self.done, {}\n",
    "             else: # ball moving around\n",
    "     #           if random.random() >= 0: \n",
    "                #self.L_reward = .1 #(.9 + 1.1 * math.cos(((self.LeftPaddle_center[1] + 300)/95.493)))\n",
    "                #self.R_reward = .1 #(.9 + 1.1 * math.cos(((self.RightPaddle_center[1] + 300)/95.493)))\n",
    "                #print('activate3')\n",
    "                #self.L_score += self.L_reward\n",
    "                #self.R_score += self.R_reward\n",
    "                self.time += 1\n",
    "                if self.velocity[0] <= 0: # ball moving left\n",
    "                    self.R_reward = .1\n",
    "                    if self.LeftPaddle_center[1] >= 140 and \\\n",
    "                            self.LeftPaddle_center[1] <= (HEIGHT - 140):\n",
    "                                self.L_reward = .2   \n",
    "                    else:\n",
    "                        self.L_reward = -.1\n",
    "                   \n",
    "                           \n",
    "                \n",
    "                    if Leftwallmove == True:\n",
    "    #                    print('me6')\n",
    "                        self.LeftPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = LeftNoMove_next_state\n",
    "                        next_L_state = LeftNoMove_next_state\n",
    "                        self.L_reward = -2\n",
    "                      \n",
    "                        self.L_score += self.L_reward\n",
    "                      \n",
    "                \n",
    "                    else:\n",
    "                        self.L_score += self.L_reward\n",
    "                    if Rightwallmove == True:\n",
    "    #                    print('me7')\n",
    "                        self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = RightNoMove_next_state\n",
    "                        next_R_state = RightNoMove_next_state\n",
    "                        self.R_reward = -2\n",
    "\n",
    "                        self.R_score += self.R_reward\n",
    "\n",
    "                    else:\n",
    "                        self.R_score += self.R_reward\n",
    "\n",
    "                    return next_L_state, next_R_state, self.L_reward, self.R_reward, self.done, {}\n",
    "                    \n",
    "                    \n",
    "                else: # ball moving right\n",
    "                    self.L_reward = .1\n",
    "                    if self.RightPaddle_center[1] >= 140 and \\\n",
    "                            self.RightPaddle_center[1] <= (HEIGHT - 140):\n",
    "                                self.L_reward = .1\n",
    "                                self.R_reward = 0.5\n",
    "                    else:\n",
    "                        self.R_reward = -.1\n",
    "                           \n",
    "                \n",
    "                    if Leftwallmove == True:\n",
    "    #                    print('me6')\n",
    "                        self.LeftPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = LeftNoMove_next_state\n",
    "                        next_L_state = LeftNoMove_next_state\n",
    "                        self.L_reward = -2\n",
    "                    \n",
    "                        self.L_score += self.L_reward\n",
    "                     \n",
    "                     \n",
    "                    else:\n",
    "                        self.L_score += self.L_reward\n",
    "                    if Rightwallmove == True:\n",
    "    #                    print('me7')\n",
    "                        self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = RightNoMove_next_state\n",
    "                        next_R_state = RightNoMove_next_state\n",
    "                        self.R_reward = -2\n",
    "                      \n",
    "                        self.R_score += self.R_reward\n",
    "                      \n",
    "                    else:\n",
    "                        self.R_score += self.R_reward\n",
    "\n",
    "                    return next_L_state, next_R_state, self.L_reward, self.R_reward, self.done, {} \n",
    "                    \n",
    "        else:#if round is done\n",
    "            if self.steps_beyond_done is None:\n",
    "                # Ball just entered a goal!\n",
    "                self.steps_beyond_done = 0\n",
    "                self.time += 1\n",
    "                if self.Ball_center[0] < -5:\n",
    "                    self.L_reward = -.1 * abs(self.LeftPaddle_center[1] - self.Ball_center[1])\n",
    "                    #print(self.L_reward)\n",
    "                    #self.R_reward = 1\n",
    "                    self.R_reward = 10.0 * (1 - (abs(self.RightPaddle_center[1] - 300) / 245))\n",
    "                    #print('activate4')\n",
    "                    \n",
    "                    if Leftwallmove == True:\n",
    "            #          print('me6')\n",
    "                        self.LeftPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = LeftNoMove_next_state\n",
    "                        next_L_state = LeftNoMove_next_state\n",
    "                        self.L_reward = -.1 * abs(self.LeftPaddle_center[1] - self.Ball_center[1])\n",
    "                   \n",
    "                        self.L_score += self.L_reward\n",
    "                  \n",
    "                    else:\n",
    "                        self.L_score += self.L_reward\n",
    "                        \n",
    "                    if Rightwallmove == True:\n",
    "                #        print('me7')\n",
    "                        self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = RightNoMove_next_state\n",
    "                        next_R_state = RightNoMove_next_state\n",
    "                        self.R_reward = -2\n",
    "                        self.R_score += self.R_reward\n",
    "                    else:\n",
    "                        self.R_score += self.R_reward\n",
    "                        \n",
    "                    return next_L_state, next_R_state, self.L_reward, self.R_reward, self.done, {} \n",
    "                \n",
    "                elif self.Ball_center[0] > WIDTH + 5:\n",
    "                    self.R_reward = -.1 * abs(self.RightPaddle_center[1] - self.Ball_center[1])\n",
    "                    #print(self.L_reward)\n",
    "                    #self.R_reward = 1\n",
    "                    self.L_reward = 10.0 * (1 - (abs(self.LeftPaddle_center[1] - 300) / 245))\n",
    "                    #print('activate4')\n",
    "                    \n",
    "                    if Leftwallmove == True:\n",
    "            #          print('me6')\n",
    "                        self.LeftPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = LeftNoMove_next_state\n",
    "                        next_L_state = LeftNoMove_next_state\n",
    "                        self.L_reward = -2\n",
    "                   \n",
    "                        self.L_score += self.L_reward\n",
    "                   \n",
    "                    else:\n",
    "                        self.L_score += self.L_reward\n",
    "                        \n",
    "                    if Rightwallmove == True:\n",
    "                #        print('me7')\n",
    "                        self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = RightNoMove_next_state\n",
    "                        next_R_state = RightNoMove_next_state\n",
    "                        self.R_reward = -.1 * abs(self.RightPaddle_center[1] - self.Ball_center[1])\n",
    "                        self.R_score += self.R_reward\n",
    "                    else:\n",
    "                        self.R_score += self.R_reward\n",
    "                        \n",
    "                    return next_L_state, next_R_state, self.L_reward, self.R_reward, self.done, {}\n",
    "                else:\n",
    "                    if self.steps_beyond_done == 0:\n",
    "                        logger.warn(\"You are calling 'step()' even though this  environment has already returned 'done = True' -- any further steps are undefined behavior.\")\n",
    "                    self.steps_beyond_done += 1\n",
    "                    self.L_reward = 0.0\n",
    "                    self.R_reward = 0.0\n",
    "                    self.time += 1\n",
    "                    #print('activate6')\n",
    "                    if Leftwallmove == True:\n",
    "            #          print('me6')\n",
    "                        self.LeftPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = LeftNoMove_next_state\n",
    "                        next_L_state = LeftNoMove_next_state\n",
    "                        self.L_reward = 0\n",
    "                        self.L_score += self.L_reward\n",
    "                    else:\n",
    "                        self.L_score += self.L_reward\n",
    "                        \n",
    "                    if Rightwallmove == True:\n",
    "                #        print('me7')\n",
    "                        self.RightPaddle_center[1], self.Ball_center[0],\\\n",
    "                            self.Ball_center[1], self.Ball_speed, self.velocity[0], self.velocity[1] = RightNoMove_next_state\n",
    "                        next_R_state = RightNoMove_next_state\n",
    "                        self.R_reward = 0\n",
    "                        self.R_score += self.R_reward\n",
    "                    else:\n",
    "                        self.R_score += self.R_reward\n",
    "                        \n",
    "                    return next_L_state, next_R_state, self.L_reward, self.R_reward, self.done, {}\n",
    "    \n",
    "\n",
    "    \n",
    "    def reset(self):\n",
    "        self.LeftPaddle_center = np.array([50, (HEIGHT/2)])\n",
    "        self.RightPaddle_center = np.array([(WIDTH-50), (HEIGHT/2)])\n",
    "        self.Ball_center = np.array([WIDTH / 2, HEIGHT / 2])\n",
    "        self.Ball_speed = 8.0\n",
    "        self.x_vel = random.uniform(-1, 1)\n",
    "        self.y_vel = random.uniform(-1, 1)\n",
    "        self.velocity = np.array([self.x_vel / math.sqrt((self.x_vel ** 2)+(self.y_vel ** 2)),\n",
    "                                  self.y_vel / math.sqrt((self.x_vel ** 2)+(self.y_vel ** 2))])  # <== [VelX, VelY]\n",
    "                                # Doing this gives the velocity vector with the speed being 1, or unified. \n",
    "        self.LeftPaddle_speed = 0\n",
    "        self.RightPaddle_speed = 0\n",
    "        self.L_state = self.LeftPaddle_center[1], self.Ball_center[0], self.Ball_center[1],\\\n",
    "                        self.Ball_speed, self.velocity[0], self.velocity[1]\n",
    "        self.R_state = self.RightPaddle_center[1], self.Ball_center[0], self.Ball_center[1],\\\n",
    "                         self.Ball_speed, self.velocity[0], self.velocity[1]\n",
    "        \n",
    "        #self.time = 0\n",
    "        self.L_bounce_tally = 0\n",
    "        self.R_bounce_tally = 0\n",
    "        self.L_score = 0\n",
    "        self.R_score = 0\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.R_state)\n",
    "        \n",
    "    def Paddle_Bounce(self):\n",
    "        if self.Ball_center[0] >= (self.LeftPaddle_area[0] - self.Ball_radius) and \\\n",
    "            self.Ball_center[0] <= (self.LeftPaddle_area[1] + 1.0 * self.Ball_radius) and \\\n",
    "            self.Ball_center[1] >= (self.LeftPaddle_area[2] - 2 * self.Ball_radius) and \\\n",
    "            self.Ball_center[1] <= (self.LeftPaddle_area[3] + 2 * self.Ball_radius):\n",
    "                diff = self.LeftPaddle_center[1] - self.Ball_center[1]\n",
    "                self.Ball_center[0] -= self.Ball_speed * self.velocity[0]\n",
    "                self.velocity[0] = -1 * self.velocity[0]\n",
    "                self.velocity[1] = -diff/45\n",
    "                if self.Ball_speed < self.MaxSpeed:\n",
    "                    self.Ball_speed += .2\n",
    "              #      print(self.Ball_center, self.velocity[0])\n",
    "                self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                  (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "                #print('Left Paddle Bounce at (x,y) = ', self.Ball_center)\n",
    "        \n",
    "        if self.Ball_center[0] >= (self.RightPaddle_area[0] - self.Ball_radius) and \\\n",
    "            self.Ball_center[0] <= (self.RightPaddle_area[1] + self.Ball_radius) and \\\n",
    "            self.Ball_center[1] >= (self.RightPaddle_area[2] - 2.0 * self.Ball_radius) and \\\n",
    "            self.Ball_center[1] <= (self.RightPaddle_area[3] + 2.0 * self.Ball_radius):\n",
    "                diff = self.RightPaddle_center[1] - self.Ball_center[1]\n",
    "                self.Ball_center[0] -= self.Ball_speed * self.velocity[0]\n",
    "           #     print(self.Ball_center, self.velocity[0])\n",
    "                self.velocity[0] = -1 * self.velocity[0]\n",
    "                self.velocity[1] = -diff/45\n",
    "                if self.Ball_speed < self.MaxSpeed:\n",
    "                    self.Ball_speed += .2\n",
    "                self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                  (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "                #print('Right Paddle Bounce at (x,y) = ', self.Ball_center)\n",
    "        \n",
    "        else:\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                  (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            \n",
    "    def TandB_Bounce(self):\n",
    "        if self.Ball_center[1] <= self.Ball_radius: # Ball bounces on top\n",
    "            self.velocity[0] = self.velocity[0] * (random.random() + 1)\n",
    "            self.velocity[1] = -1.0 * self.velocity[1]\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                  (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            self.Ball_center[1] = 5\n",
    "            \n",
    "        elif self.Ball_center[1] >= (HEIGHT - self.Ball_radius): #Ball bounces on bottom   \n",
    "            self.velocity[0] = self.velocity[0] * (random.random() + 1)\n",
    "            self.velocity[1] = -1.0 * self.velocity[1]\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                  (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            self.Ball_center[1] = (HEIGHT - 5)\n",
    "            \n",
    "        else: # Not bouncing, so just returns the current velocity\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                  (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            \n",
    "    def SideWall_Bounce(self): \n",
    "        if self.Ball_center[0] <= self.Ball_radius and self.Ball_center[1] <= 160 + self.Ball_radius:\n",
    "            self.velocity[0] = -1.0 * self.velocity[0]\n",
    "            self.velocity[1] = self.velocity[1] * (random.random() + 1)\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            self.Ball_center[0] = 5\n",
    "        \n",
    "        elif self.Ball_center[0] <= self.Ball_radius and self.Ball_center[1] >= (HEIGHT - 160 - self.Ball_radius):\n",
    "            self.velocity[0] = -1.0 * self.velocity[0]\n",
    "            self.velocity[1] = self.velocity[1] * (random.random() + 1)\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            self.Ball_center[0] = 5\n",
    "        \n",
    "        elif self.Ball_center[0] >= (WIDTH - self.Ball_radius) and self.Ball_center[1] <= 160: # + self.Ball_radius:\n",
    "            self.velocity[0] = -1.0 * self.velocity[0]\n",
    "            self.velocity[1] = self.velocity[1] * (random.random() + 1)\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            self.Ball_center[0] = WIDTH - 5\n",
    "            \n",
    "            \n",
    "        elif self.Ball_center[0] >= (WIDTH - self.Ball_radius) and self.Ball_center[1] >= HEIGHT - 160: #- self.Ball_radius:\n",
    "            self.velocity[0] = -1.0 * self.velocity[0]\n",
    "            self.velocity[1] = self.velocity[1] * (random.random() + 1)\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                 (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            self.Ball_center[0] = WIDTH - 5\n",
    "            \n",
    "            \n",
    "#         elif self.Ball_center[0] >= (WIDTH - self.Ball_radius) and self.Ball_center[1] >= HEIGHT - 160 - self.Ball_radius:\n",
    "#             self.velocity[0] = -1.0 * self.velocity[0]\n",
    "#             self.velocity[1] = self.velocity[1] * (random.random() + 1)\n",
    "#             self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "#                                 (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "#             self.Ball_center[0] = WIDTH - 5  \n",
    "            \n",
    "        else:\n",
    "            self.velocity = np.array([(self.velocity[0] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2))),\n",
    "                                (self.velocity[1] / math.sqrt((self.velocity[0] ** 2) + (self.velocity[1] ** 2)))])\n",
    "            \n",
    "#     def RightPaddle_KeyPress(self):\n",
    "#         self.RightPaddle_speed = 0\n",
    "#         keys = pygame.key.get_pressed()\n",
    "#         if keys[pygame.K_UP]:\n",
    "#             self.RightPaddle_speed = -1 * self.Paddle_vel\n",
    "#         if keys[pygame.K_DOWN]:\n",
    "#             self.RightPaddle_speed = self.Paddle_vel\n",
    "#         self.RightPaddle_center += self.RightPaddle_speed\n",
    "    \n",
    "    def Render(self, L_state, R_state):\n",
    "        class TandBWall(pygame.sprite.Sprite):\n",
    "            def __init__(self, y_pos):\n",
    "                super().__init__()\n",
    "                self.width = WIDTH\n",
    "                self.height = 5\n",
    "                self.image = pygame.Surface([self.width, self.height])\n",
    "                self.image.fill((255, 255, 0))\n",
    "                self.rect = self.image.get_rect()\n",
    "                self.screenheight = HEIGHT\n",
    "                self.screenwidth = WIDTH\n",
    "                self.rect.centerx = WIDTH / 2\n",
    "                self.rect.y = y_pos\n",
    "        \n",
    "        class sideWall(pygame.sprite.Sprite):\n",
    "            def __init__(self, x_pos, height, y_pos):\n",
    "                super().__init__()\n",
    "                self.width = 5\n",
    "                self.height = height\n",
    "                self.image = pygame.Surface([self.width, self.height])\n",
    "                self.image.fill((255, 255, 0))\n",
    "                self.rect = self.image.get_rect()\n",
    "                self.screenheight = HEIGHT\n",
    "                self.screenwidth = WIDTH\n",
    "                self.rect.centery = y_pos\n",
    "                self.rect.x = x_pos\n",
    "        \n",
    "        class ball(pygame.sprite.Sprite):\n",
    "            # ball constructor\n",
    "            def __init__(self, x, y, speed, xvel, yvel):\n",
    "                super().__init__()\n",
    "                self.image = pygame.Surface([10, 10])\n",
    "                pygame.draw.circle(self.image, (255, 255, 255), (5,5), 5)\n",
    "                self.rect = self.image.get_rect()\n",
    "                self.screenheight = HEIGHT\n",
    "                self.screenwidth = WIDTH\n",
    "                self.speed = speed\n",
    "                self.x_pos = x\n",
    "                self.y_pos = y\n",
    "                self.x_vel = xvel\n",
    "                self.y_vel = yvel\n",
    "                self.maxSpeed = 25\n",
    "                self.velocity = np.array([xvel, yvel])\n",
    "                self.width = 10\n",
    "                self.height = 10\n",
    "            \n",
    "                self.rect.center = np.array([x, y])\n",
    "        \n",
    "        class paddle(pygame.sprite.Sprite):\n",
    "            # paddle constructor\n",
    "            def __init__(self, x_pos, y_pos, vel):\n",
    "                super().__init__()\n",
    "                self.width = 10\n",
    "                self.height = 90\n",
    "                self.image = pygame.Surface([self.width, self.height])\n",
    "                self.image.fill((255, 0, 0))\n",
    "                self.speed = 0\n",
    "                #self.up = upKey\n",
    "                #self.down = downKey\n",
    "                self.vel = vel\n",
    "                self.rect = self.image.get_rect()\n",
    "                self.screenheight = HEIGHT\n",
    "                self.screenwidth = WIDTH\n",
    "\n",
    "                self.rect.centery = y_pos\n",
    "                self.rect.centerx = x_pos\n",
    "        \n",
    "        all_sprites = pygame.sprite.Group()\n",
    "        TandBWalls = pygame.sprite.Group()\n",
    "        balls = pygame.sprite.Group()\n",
    "        sideWalls = pygame.sprite.Group()\n",
    "        \n",
    "        TopWall = TandBWall(-1)                                          # make top wall\n",
    "        TandBWalls.add(TopWall)\n",
    "        \n",
    "        BottomWall = TandBWall(HEIGHT-4)                                 # make bottom wall\n",
    "        TandBWalls.add(BottomWall)\n",
    "        \n",
    "        \n",
    "       \n",
    "        ball1 = ball(R_state[1], R_state[2], R_state[3], R_state[4], R_state[5])      # make ball\n",
    "        all_sprites.add(ball1)\n",
    "        balls.add(ball1)\n",
    "        \n",
    "        paddle1 = paddle(50, L_state[0], 5)                  # make left paddle\n",
    "        all_sprites.add(paddle1)\n",
    "        \n",
    "        paddle2 = paddle((WIDTH-50), R_state[0], 5)      # make right paddle\n",
    "        all_sprites.add(paddle2)\n",
    "        \n",
    "        TleftWall = sideWall(-1, 160, 80)                                # make top left wall\n",
    "        sideWalls.add(TleftWall)\n",
    "\n",
    "        TrightWall = sideWall(WIDTH-4, 160, 80)                          # make top right wall was WIDTH-4, 160, 80\n",
    "        sideWalls.add(TrightWall)\n",
    "\n",
    "        BleftWall = sideWall(-1, 160, HEIGHT-80)                         # make bottom left wall\n",
    "        sideWalls.add(BleftWall)\n",
    "\n",
    "        BrightWall = sideWall(WIDTH-4, 160, HEIGHT-80)                   # make bottom right wall was WIDTH-4, 160, HEIGHT-80\n",
    "        sideWalls.add(BrightWall)\n",
    "        \n",
    "        screen.fill((0, 0, 0))\n",
    "        all_sprites.draw(screen)\n",
    "        TandBWalls.draw(screen)\n",
    "        sideWalls.draw(screen)\n",
    "        pygame.display.flip()\n",
    "        time.sleep(.02)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DQN Agent for L_paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DQNLAgent():\n",
    "    def __init__(self, state_size, action_size, epsilon):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.memory = deque(maxlen = 2000) #List of max length 2000 that I can add or remove data points to\n",
    "        \n",
    "        self.gamma = 0.95 # How much to discount future reward\n",
    "        \n",
    "        self.epsilon = epsilon # Explore primarily first\n",
    "        self.epsilon_decay = 0.999 # After time, start to exploit more\n",
    "        self.epsilon_min = 0.05 # Min ratio of eploration / exploitation is 5%\n",
    "        \n",
    "        self.learning_rate = 0.001 # Stocastic gradient decent step size\n",
    "        \n",
    "        self.zeta = .125 # impacts the training_model, .125 was original\n",
    "        \n",
    "        self.model = self._build_model()    # Builds the model for the prediction agent\n",
    "        self.target_model = self._build_model()    #Builds the counter_intuitive model for tracking \"target values\".\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(36, input_dim = self.state_size, activation = 'relu', kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        model.add(Dense(96, activation = 'relu', kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "#         model.add(Dense(96, activation = 'relu', kernel_initializer=tf.contrib.layers.xavier_initializer())) # L_model doesnt have this\n",
    "    #    model.add(Dense(96, activation = 'relu',kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        model.add(Dense(48, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu',kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        model.add(Dense(self.action_size, activation = 'linear', kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "    #    model.add(Dense(self.action_size, activation = 'sigmoid', kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        \n",
    "    #    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr = self.learning_rate)) # might change mse to 'categorical_crossentropy'  'mse'\n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        #print(model)\n",
    "        return model\n",
    "        \n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) # Chooses a random action to perform (0 or 1)  \n",
    "        act_values = self.model.predict(state) # Gets the action values by running the current state through the current model\n",
    "#        print('act_values =', act_values)\n",
    "        return np.argmax(act_values[0]) # returns the action with the highest act value\n",
    "      \n",
    "    def replay(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size) # Randomly samples from the memory 32 data points\n",
    "        #print(len(agent.memory))\n",
    "        #print(agent.memory)\n",
    "   #     for state, action, reward, next_state, done in samples:\n",
    "            \n",
    "        for sample in samples:\n",
    "            state, action, reward, next_state, done = sample\n",
    "#             if reward > 0 and \\\n",
    "#                 reward <= 3:\n",
    "#                 print('reward used in training between 0 and 3 =', reward)\n",
    "#             if reward > 3:\n",
    "#                 print('reward used in training above 3 =', reward)\n",
    "#             if reward < 0:\n",
    "       #     print('reward used in training =', reward)\n",
    "#             if sample[2] != 1.0:\n",
    "#                 print('reward =', sample[2])\n",
    "            #if sample[2] == 0:\n",
    "            #    print(sample[0])\n",
    "            target = self.target_model.predict(state)\n",
    "     #       print('target based on state =', target)\n",
    "     #       print(state[0][0])\n",
    "     #       print('action performed =', action)\n",
    "     #       print(next_state[0][0])\n",
    "            #print(target)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "                #print('was done, target[0][action]', target[0][action])\n",
    "                #print('done reward =', reward)\n",
    "                #print('end of sample, should run fit')\n",
    "            #target_f = self.model.predict(state)\n",
    "            #x = next_state\n",
    "            #print(type(x))\n",
    "            #print(x)\n",
    "            #print(self.model.predict(x))\n",
    "            else:\n",
    "                #print('was not done')\n",
    "                #if not reward in range(0, 2):\n",
    "                \n",
    "     #           print('current reward =', reward)\n",
    "             #   print('action = ', action)\n",
    "     #           print('target_model_predict based on next state =',self.target_model.predict(next_state)[0])\n",
    "                Q_future = max(self.target_model.predict(next_state)[0])\n",
    "     #           print('Q_future, should be max of predict on next state =', Q_future)\n",
    "                target[0][action] = reward + (Q_future * self.gamma)\n",
    "     #           print('target[0][action], should be reward + Q_f*gamma =', target[0][action])\n",
    "     #           print('target, should have changed 1 value of target =', target)\n",
    "                #print('end of sample, should run fit')\n",
    "                #print('was not done, target =', target)\n",
    "                #target = (reward + self.gamma * np.amax(self.model.predict(next_state)))\n",
    "                #print('target =', target)\n",
    "            #target_f = self.model.predict(state) #taget_f is 'future reward'\n",
    "            #print('target_f =', target_f)\n",
    "            #target_f[0][action] = target\n",
    "            #print('performed learning')\n",
    "            self.model.fit(state, target, epochs = 1, verbose = 0)\n",
    "     #       print('ran fit')\n",
    "            \n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        #print('weights =', weights)\n",
    "        #print('target_weights =', target_weights)\n",
    "#         for i in range(len(target_weights)):\n",
    "#             target_weights[i] = weights[i] * self.zeta + target_weights[i] * (1 - self.zeta)\n",
    "#         self.target_model.set_weights(target_weights)\n",
    "        target_weights = weights\n",
    "        self.target_model.set_weights(target_weights)\n",
    "        #print('performed target_train')\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make DQN Agent for R_Paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNRAgent():\n",
    "    def __init__(self, state_size, action_size, epsilon):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.memory = deque(maxlen = 2000) #List of max length 2000 that I can add or remove data points to\n",
    "        \n",
    "        self.gamma = 0.95 # How much to discount future reward\n",
    "        \n",
    "        self.epsilon = epsilon # Explore primarily first\n",
    "        self.epsilon_decay = 0.999 # After time, start to exploit more\n",
    "        self.epsilon_min = 0.05 # Min ratio of eploration / exploitation is 5%\n",
    "        \n",
    "        self.learning_rate = 0.001 # Stocastic gradient decent step size\n",
    "        \n",
    "        self.zeta = .125 # impacts the training_model, .125 was original\n",
    "        \n",
    "        self.model = self._build_model()    # Builds the model for the prediction agent\n",
    "        self.target_model = self._build_model() # model does the actual predictions on what action to take, and\n",
    "                                                   # the target model tracks what action we want our model to take\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(36, input_dim = self.state_size, activation = 'relu', kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        model.add(Dense(96, activation = 'relu', kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        model.add(Dense(96, activation = 'relu', kernel_initializer=tf.contrib.layers.xavier_initializer())) # L_model doesnt have this\n",
    "    #    model.add(Dense(96, activation = 'relu',kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        model.add(Dense(48, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu',kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        model.add(Dense(self.action_size, activation = 'linear', kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "    #    model.add(Dense(self.action_size, activation = 'sigmoid', kernel_initializer=tf.contrib.layers.xavier_initializer()))\n",
    "        \n",
    "    #    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr = self.learning_rate)) # might change mse to 'categorical_crossentropy'  'mse'\n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        #print(model)\n",
    "        return model\n",
    "     \n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size) # Chooses a random action to perform (0 or 1)  \n",
    "        act_values = self.model.predict(state) # Gets the action values by running the current state through the current model\n",
    "#        print('act_values =', act_values)\n",
    "        return np.argmax(act_values[0]) # returns the action with the highest act value\n",
    "      \n",
    "    def replay(self, batch_size):\n",
    "        samples = random.sample(self.memory, batch_size) # Randomly samples from the memory 32 data points\n",
    "        #print(len(agent.memory))\n",
    "        #print(agent.memory)\n",
    "   #     for state, action, reward, next_state, done in samples:\n",
    "            \n",
    "        for sample in samples:\n",
    "            state, action, reward, next_state, done = sample\n",
    "#             if reward > 0 and \\\n",
    "#                 reward <= 3:\n",
    "#                 print('reward used in training between 0 and 3 =', reward)\n",
    "#             if reward > 3:\n",
    "#                 print('reward used in training above 3 =', reward)\n",
    "#             if reward < 0:\n",
    "       #     print('reward used in training =', reward)\n",
    "#             if sample[2] != 1.0:\n",
    "#                 print('reward =', sample[2])\n",
    "            #if sample[2] == 0:\n",
    "            #    print(sample[0])\n",
    "            target = self.target_model.predict(state)\n",
    "     #       print('target based on state =', target)\n",
    "     #       print(state[0][0])\n",
    "     #       print('action performed =', action)\n",
    "     #       print(next_state[0][0])\n",
    "            #print(target)\n",
    "            if done:\n",
    "                target[0][action] = reward\n",
    "                #print('was done, target[0][action]', target[0][action])\n",
    "                #print('done reward =', reward)\n",
    "                #print('end of sample, should run fit')\n",
    "            #target_f = self.model.predict(state)\n",
    "            #x = next_state\n",
    "            #print(type(x))\n",
    "            #print(x)\n",
    "            #print(self.model.predict(x))\n",
    "            else:\n",
    "                #print('was not done')\n",
    "                #if not reward in range(0, 2):\n",
    "                \n",
    "     #           print('current reward =', reward)\n",
    "             #   print('action = ', action)\n",
    "     #           print('target_model_predict based on next state =',self.target_model.predict(next_state)[0])\n",
    "                Q_future = max(self.target_model.predict(next_state)[0])\n",
    "     #           print('Q_future, should be max of predict on next state =', Q_future)\n",
    "                target[0][action] = reward + (Q_future * self.gamma)\n",
    "     #           print('target[0][action], should be reward + Q_f*gamma =', target[0][action])\n",
    "     #           print('target, should have changed 1 value of target =', target)\n",
    "                #print('end of sample, should run fit')\n",
    "                #print('was not done, target =', target)\n",
    "                #target = (reward + self.gamma * np.amax(self.model.predict(next_state)))\n",
    "                #print('target =', target)\n",
    "            #target_f = self.model.predict(state) #taget_f is 'future reward'\n",
    "            #print('target_f =', target_f)\n",
    "            #target_f[0][action] = target\n",
    "            #print('performed learning')\n",
    "            self.model.fit(state, target, epochs = 1, verbose = 0)\n",
    "     #       print('ran fit')\n",
    "            \n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def target_train(self):\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        #print('weights =', weights)\n",
    "        #print('target_weights =', target_weights)\n",
    "#         for i in range(len(target_weights)):\n",
    "#             target_weights[i] = weights[i] * self.zeta + target_weights[i] * (1 - self.zeta)\n",
    "#         self.target_model.set_weights(target_weights)\n",
    "        target_weights = weights\n",
    "        self.target_model.set_weights(target_weights)\n",
    "        #print('performed target_train')\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/500, time: 257, L_score: 88.14489795918308, R_score: 69.62822960776585, eL: 0.05, eR: 0.05\n",
      "episode: 1/500, time: 355, L_score: 102.74489795918323, R_score: 25.67365864456069, eL: 0.05, eR: 0.05\n",
      "episode: 2/500, time: 84, L_score: 7.819675854483824, R_score: 15.95102040816325, eL: 0.05, eR: 0.05\n",
      "episode: 3/500, time: 469, L_score: 65.4285714285719, R_score: 295.1545004312725, eL: 0.05, eR: 0.05\n",
      "episode: 4/500, time: 1673, L_score: 294.9489795918333, R_score: 1134.9491737417404, eL: 0.05, eR: 0.05\n",
      "episode: 5/500, time: 364, L_score: 144.95237362360595, R_score: 87.32244897959114, eL: 0.05, eR: 0.05\n",
      "episode: 6/500, time: 339, L_score: 100.14489795918325, R_score: 103.0953154530576, eL: 0.05, eR: 0.05\n",
      "episode: 7/500, time: 253, L_score: 64.5903732416457, R_score: 358.5306122449015, eL: 0.05, eR: 0.05\n",
      "episode: 8/500, time: 265, L_score: 88.93673469387696, R_score: 266.1009810888077, eL: 0.05, eR: 0.05\n",
      "episode: 9/500, time: 148, L_score: 15.208163265306087, R_score: 57.54883312471696, eL: 0.05, eR: 0.05\n",
      "episode: 10/500, time: 1215, L_score: -1.2816326530653308, R_score: 270.69756196242184, eL: 0.05, eR: 0.05\n",
      "episode: 11/500, time: 50, L_score: 5.60816326530612, R_score: 216.80287267653512, eL: 0.05, eR: 0.05\n",
      "episode: 12/500, time: 1901, L_score: 276.59623760345653, R_score: 893.2673469387908, eL: 0.05, eR: 0.05\n",
      "episode: 13/500, time: 626, L_score: 139.76530612244886, R_score: 321.70542871655215, eL: 0.05, eR: 0.05\n",
      "episode: 14/500, time: 406, L_score: 42.72058823238204, R_score: 264.342857142857, eL: 0.05, eR: 0.05\n",
      "episode: 15/500, time: 860, L_score: 149.67346938775438, R_score: 369.11523722651964, eL: 0.05, eR: 0.05\n",
      "episode: 16/500, time: 163, L_score: 23.336734693877602, R_score: 10.633855019548946, eL: 0.05, eR: 0.05\n",
      "episode: 17/500, time: 1072, L_score: 301.06530612245047, R_score: 469.91058918122866, eL: 0.05, eR: 0.05\n",
      "episode: 18/500, time: 961, L_score: 230.9285714285711, R_score: 251.23131515769037, eL: 0.05, eR: 0.05\n",
      "episode: 19/500, time: 1478, L_score: 293.0530612244887, R_score: 592.100709355982, eL: 0.05, eR: 0.05\n",
      "episode: 20/500, time: 50, L_score: 6.732653061224488, R_score: 116.6155288040263, eL: 0.05, eR: 0.05\n",
      "episode: 21/500, time: 717, L_score: 308.6285714285706, R_score: 167.8453193571588, eL: 0.05, eR: 0.05\n",
      "episode: 22/500, time: 374, L_score: 103.0653061224483, R_score: 385.94270861402657, eL: 0.05, eR: 0.05\n",
      "episode: 23/500, time: 437, L_score: 112.68571428571403, R_score: 53.9949253220244, eL: 0.05, eR: 0.05\n",
      "episode: 24/500, time: 1074, L_score: 305.4653061224512, R_score: 385.5167190790137, eL: 0.05, eR: 0.05\n",
      "episode: 25/500, time: 103, L_score: -0.018855236360749927, R_score: 18.055102040816305, eL: 0.05, eR: 0.05\n",
      "episode: 26/500, time: 164, L_score: 23.332653061224544, R_score: 8.157973428363922, eL: 0.05, eR: 0.05\n",
      "episode: 27/500, time: 356, L_score: -49.14385996145894, R_score: -33.514285714285606, eL: 0.05, eR: 0.05\n",
      "episode: 28/500, time: 302, L_score: 36.094647846358725, R_score: 38.77142857142873, eL: 0.05, eR: 0.05\n",
      "episode: 29/500, time: 316, L_score: 94.24897959183598, R_score: 64.67391237908652, eL: 0.05, eR: 0.05\n",
      "episode: 30/500, time: 471, L_score: 29.31224489795917, R_score: 340.6788452676216, eL: 0.05, eR: 0.05\n",
      "episode: 31/500, time: 1413, L_score: 212.72167801102478, R_score: 652.5224489795941, eL: 0.05, eR: 0.05\n",
      "episode: 32/500, time: 345, L_score: 29.43673469387764, R_score: 130.35110083249933, eL: 0.05, eR: 0.05\n",
      "episode: 33/500, time: 297, L_score: 34.82040816326544, R_score: 65.3839438566638, eL: 0.05, eR: 0.05\n",
      "episode: 34/500, time: 827, L_score: 328.02857142856874, R_score: 399.51591665983676, eL: 0.05, eR: 0.05\n",
      "episode: 35/500, time: 430, L_score: 160.9244897959176, R_score: 421.60826940993957, eL: 0.05, eR: 0.05\n",
      "episode: 36/500, time: 266, L_score: 26.938431877459465, R_score: 23.914285714285725, eL: 0.05, eR: 0.05\n",
      "episode: 37/500, time: 1248, L_score: 228.85714285714002, R_score: 786.8983011297387, eL: 0.05, eR: 0.05\n",
      "episode: 38/500, time: 293, L_score: 36.75306122448992, R_score: 60.59674544813042, eL: 0.05, eR: 0.05\n",
      "episode: 39/500, time: 561, L_score: 126.02040816326517, R_score: 570.7289102702271, eL: 0.05, eR: 0.05\n",
      "episode: 40/500, time: 97, L_score: 11.636734693877532, R_score: 129.40519101661084, eL: 0.05, eR: 0.05\n",
      "episode: 41/500, time: 229, L_score: -9.255102040816219, R_score: 19.206359864911416, eL: 0.05, eR: 0.05\n",
      "episode: 42/500, time: 479, L_score: 119.9020408163264, R_score: 305.3351049049518, eL: 0.05, eR: 0.05\n",
      "episode: 43/500, time: 1296, L_score: 73.53236489509503, R_score: 107.74693877550902, eL: 0.05, eR: 0.05\n",
      "episode: 44/500, time: 421, L_score: 57.92857142857182, R_score: 88.36684317297693, eL: 0.05, eR: 0.05\n",
      "episode: 45/500, time: 324, L_score: 89.5508285458329, R_score: 282.1591836734707, eL: 0.05, eR: 0.05\n",
      "episode: 46/500, time: 83, L_score: 8.450810835409875, R_score: 15.442857142857129, eL: 0.05, eR: 0.05\n",
      "episode: 47/500, time: 252, L_score: 84.74897959183616, R_score: 176.2715504467932, eL: 0.05, eR: 0.05\n",
      "episode: 48/500, time: 712, L_score: 38.840816326530714, R_score: 152.73878902505916, eL: 0.05, eR: 0.05\n",
      "episode: 49/500, time: 1014, L_score: 346.6571428571425, R_score: 453.98996849170334, eL: 0.05, eR: 0.05\n",
      "episode: 50/500, time: 1048, L_score: 140.56938775510267, R_score: 526.4682096233541, eL: 0.05, eR: 0.05\n",
      "episode: 51/500, time: 223, L_score: 25.336734693877574, R_score: 86.1447505221683, eL: 0.05, eR: 0.05\n",
      "episode: 52/500, time: 852, L_score: -11.751020408163555, R_score: 61.812186167299906, eL: 0.05, eR: 0.05\n",
      "episode: 53/500, time: 425, L_score: 153.72040816326495, R_score: 540.3591011622592, eL: 0.05, eR: 0.05\n",
      "episode: 54/500, time: 355, L_score: 152.75306122448933, R_score: 196.95741133463744, eL: 0.05, eR: 0.05\n",
      "episode: 55/500, time: 1833, L_score: 384.18097109087523, R_score: 676.8591836734821, eL: 0.05, eR: 0.05\n",
      "episode: 56/500, time: 281, L_score: 88.76938775510126, R_score: 376.2379267881806, eL: 0.05, eR: 0.05\n",
      "episode: 57/500, time: 419, L_score: 105.25306122448944, R_score: 274.8109035489433, eL: 0.05, eR: 0.05\n",
      "episode: 58/500, time: 431, L_score: 110.7285714285707, R_score: 323.9554083829788, eL: 0.05, eR: 0.05\n",
      "episode: 59/500, time: 349, L_score: 153.65306122448897, R_score: 174.97030161056668, eL: 0.05, eR: 0.05\n",
      "episode: 60/500, time: 964, L_score: 80.01519096984717, R_score: 496.0857142857164, eL: 0.05, eR: 0.05\n",
      "episode: 61/500, time: 348, L_score: 101.4326530612241, R_score: 56.985070549491844, eL: 0.05, eR: 0.05\n",
      "episode: 62/500, time: 50, L_score: 5.816326530612243, R_score: 16.64700515405537, eL: 0.05, eR: 0.05\n",
      "episode: 63/500, time: 946, L_score: 337.45714285714155, R_score: 491.8836181842078, eL: 0.05, eR: 0.05\n",
      "episode: 64/500, time: 673, L_score: 150.4775510204071, R_score: 197.00761872190492, eL: 0.05, eR: 0.05\n",
      "episode: 65/500, time: 79, L_score: 9.736734693877539, R_score: 23.125005852414432, eL: 0.05, eR: 0.05\n",
      "episode: 66/500, time: 365, L_score: 147.1254721367971, R_score: 88.23877551020335, eL: 0.05, eR: 0.05\n",
      "episode: 67/500, time: 603, L_score: 105.24489795918316, R_score: 308.7394566882922, eL: 0.05, eR: 0.05\n",
      "episode: 68/500, time: 249, L_score: 86.92857142857086, R_score: 167.38438480772635, eL: 0.05, eR: 0.05\n",
      "episode: 69/500, time: 316, L_score: 23.532653061224476, R_score: 108.10525476271943, eL: 0.05, eR: 0.05\n",
      "episode: 70/500, time: 253, L_score: 37.144897959183766, R_score: 133.40843419952137, eL: 0.05, eR: 0.05\n",
      "episode: 71/500, time: 1581, L_score: 470.9327102061498, R_score: 821.0714285714419, eL: 0.05, eR: 0.05\n",
      "episode: 72/500, time: 1530, L_score: 265.44489795918065, R_score: 601.442953718318, eL: 0.05, eR: 0.05\n",
      "episode: 73/500, time: 1812, L_score: 476.3172803032335, R_score: 627.5183673469489, eL: 0.05, eR: 0.05\n",
      "episode: 74/500, time: 94, L_score: 12.348979591836716, R_score: 524.3604624019544, eL: 0.05, eR: 0.05\n",
      "episode: 75/500, time: 157, L_score: 99.04256475090374, R_score: 23.863265306122408, eL: 0.05, eR: 0.05\n",
      "episode: 76/500, time: 769, L_score: 153.55306122449008, R_score: 187.8840728728075, eL: 0.05, eR: 0.05\n",
      "episode: 77/500, time: 228, L_score: -23.038775510203898, R_score: 220.47110096374027, eL: 0.05, eR: 0.05\n",
      "episode: 78/500, time: 1061, L_score: 281.69867209572, R_score: 513.5000000000009, eL: 0.05, eR: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 79/500, time: 819, L_score: 209.136734693873, R_score: 233.38038155694306, eL: 0.05, eR: 0.05\n",
      "episode: 80/500, time: 660, L_score: 86.52448979591752, R_score: 262.1611766114406, eL: 0.05, eR: 0.05\n",
      "episode: 81/500, time: 550, L_score: 96.5312385518856, R_score: 68.73469387755145, eL: 0.05, eR: 0.05\n",
      "episode: 82/500, time: 1074, L_score: 197.54897959183543, R_score: 226.91373018504717, eL: 0.05, eR: 0.05\n",
      "episode: 83/500, time: 989, L_score: 230.66122448979212, R_score: 295.8690269967507, eL: 0.05, eR: 0.05\n",
      "episode: 84/500, time: 468, L_score: 116.14081632653044, R_score: 333.2427396657019, eL: 0.05, eR: 0.05\n",
      "episode: 85/500, time: 739, L_score: 108.86938775510185, R_score: 255.5905276609004, eL: 0.05, eR: 0.05\n",
      "episode: 86/500, time: 482, L_score: 113.26938775510175, R_score: 202.09809439473472, eL: 0.05, eR: 0.05\n",
      "episode: 87/500, time: 655, L_score: 186.57755102040647, R_score: 303.1888996415846, eL: 0.05, eR: 0.05\n",
      "episode: 88/500, time: 366, L_score: 51.65202073659667, R_score: 43.742857142857396, eL: 0.05, eR: 0.05\n",
      "episode: 89/500, time: 78, L_score: 9.8408163265306, R_score: 23.68513789674453, eL: 0.05, eR: 0.05\n",
      "episode: 90/500, time: 1990, L_score: 578.1408163265387, R_score: 514.8300445857749, eL: 0.05, eR: 0.05\n",
      "episode: 91/500, time: 201, L_score: 80.53673469387688, R_score: 39.64969472839971, eL: 0.05, eR: 0.05\n",
      "episode: 92/500, time: 409, L_score: 207.44489795918136, R_score: 314.19939222481855, eL: 0.05, eR: 0.05\n",
      "episode: 93/500, time: 80, L_score: 4.465262940301304, R_score: 15.142857142857132, eL: 0.05, eR: 0.05\n",
      "episode: 94/500, time: 565, L_score: 119.61610788318028, R_score: 303.9632653061257, eL: 0.05, eR: 0.05\n",
      "episode: 95/500, time: 1682, L_score: 216.55735803360218, R_score: 386.48367346938807, eL: 0.05, eR: 0.05\n",
      "episode: 96/500, time: 169, L_score: 25.561224489795986, R_score: 24.433723450343994, eL: 0.05, eR: 0.05\n",
      "episode: 97/500, time: 845, L_score: 214.04489795918008, R_score: 312.91848955329687, eL: 0.05, eR: 0.05\n",
      "episode: 98/500, time: 382, L_score: 104.46530612244862, R_score: 202.12260898385995, eL: 0.05, eR: 0.05\n",
      "episode: 99/500, time: 201, L_score: 18.040533180407856, R_score: 47.634693877551214, eL: 0.05, eR: 0.05\n",
      "episode: 100/500, time: 841, L_score: 212.1448979591824, R_score: 609.9450918274855, eL: 0.05, eR: 0.05\n",
      "episode: 101/500, time: 612, L_score: 97.85590580793132, R_score: 70.10612244897932, eL: 0.05, eR: 0.05\n",
      "episode: 102/500, time: 1492, L_score: 238.77755102040553, R_score: 523.7514560235456, eL: 0.05, eR: 0.05\n",
      "episode: 103/500, time: 85, L_score: 44.073491492986236, R_score: 15.642857142857128, eL: 0.05, eR: 0.05\n",
      "episode: 104/500, time: 250, L_score: 85.43265306122393, R_score: 268.05488340918845, eL: 0.05, eR: 0.05\n",
      "episode: 105/500, time: 620, L_score: 225.53206668358575, R_score: 121.04285714285623, eL: 0.05, eR: 0.05\n",
      "episode: 106/500, time: 285, L_score: 39.85306122448984, R_score: 32.510463852641934, eL: 0.05, eR: 0.05\n",
      "episode: 107/500, time: 584, L_score: 82.24805669426941, R_score: 224.4428571428558, eL: 0.05, eR: 0.05\n",
      "episode: 108/500, time: 2167, L_score: 523.1571428571492, R_score: 671.6476381743513, eL: 0.05, eR: 0.05\n",
      "episode: 109/500, time: 2653, L_score: 485.05587448281653, R_score: 1106.2224489795944, eL: 0.05, eR: 0.05\n",
      "episode: 110/500, time: 160, L_score: 23.665306122449017, R_score: 30.23824800387156, eL: 0.05, eR: 0.05\n",
      "episode: 111/500, time: 91, L_score: 12.144897959183659, R_score: 24.51212183759125, eL: 0.05, eR: 0.05\n",
      "episode: 112/500, time: 52, L_score: 6.424489795918365, R_score: 15.783064013883449, eL: 0.05, eR: 0.05\n",
      "episode: 113/500, time: 881, L_score: 91.15714285714236, R_score: 298.1224894757907, eL: 0.05, eR: 0.05\n",
      "episode: 114/500, time: 50, L_score: 6.224489795918366, R_score: 16.544703489238834, eL: 0.05, eR: 0.05\n",
      "episode: 115/500, time: 361, L_score: 16.36122448979581, R_score: 64.13741061687415, eL: 0.05, eR: 0.05\n",
      "episode: 116/500, time: 50, L_score: 7.04081632653061, R_score: 17.86735706528888, eL: 0.05, eR: 0.05\n",
      "episode: 117/500, time: 427, L_score: 147.89496782264973, R_score: 90.03877551020368, eL: 0.05, eR: 0.05\n",
      "episode: 118/500, time: 358, L_score: 93.64403610322255, R_score: 99.37142857142818, eL: 0.05, eR: 0.05\n",
      "episode: 119/500, time: 1209, L_score: 417.761224489795, R_score: 510.44266848976815, eL: 0.05, eR: 0.05\n",
      "episode: 120/500, time: 2144, L_score: 284.1816326530554, R_score: 867.5492360628109, eL: 0.05, eR: 0.05\n",
      "episode: 121/500, time: 212, L_score: 23.93454905882893, R_score: 43.30612244897977, eL: 0.05, eR: 0.05\n",
      "episode: 122/500, time: 415, L_score: 44.53293146191045, R_score: 114.2306122448968, eL: 0.05, eR: 0.05\n",
      "episode: 123/500, time: 410, L_score: 108.41632653061191, R_score: 108.1312446550782, eL: 0.05, eR: 0.05\n",
      "episode: 124/500, time: 519, L_score: -11.304415824002561, R_score: 61.82653061224532, eL: 0.05, eR: 0.05\n",
      "episode: 125/500, time: 366, L_score: 53.94081632653079, R_score: 280.34123423796484, eL: 0.05, eR: 0.05\n",
      "episode: 126/500, time: 247, L_score: 81.53673469387697, R_score: 176.34891971564213, eL: 0.05, eR: 0.05\n",
      "episode: 127/500, time: 53, L_score: -2.5428384838762845, R_score: 11.422448979591834, eL: 0.05, eR: 0.05\n",
      "episode: 128/500, time: 454, L_score: 166.22448979591618, R_score: 519.3083233995167, eL: 0.05, eR: 0.05\n",
      "episode: 129/500, time: 793, L_score: 41.565306122449556, R_score: 671.5074651901514, eL: 0.05, eR: 0.05\n",
      "episode: 130/500, time: 876, L_score: 322.5081632653047, R_score: 617.7209883771127, eL: 0.05, eR: 0.05\n",
      "episode: 131/500, time: 713, L_score: 27.289795918367425, R_score: 329.4183189490104, eL: 0.05, eR: 0.05\n",
      "episode: 132/500, time: 1519, L_score: 421.94489795918105, R_score: 487.1765424579091, eL: 0.05, eR: 0.05\n",
      "episode: 133/500, time: 2995, L_score: 258.50729024730197, R_score: 1999.7918367346585, eL: 0.05, eR: 0.05\n",
      "episode: 134/500, time: 113, L_score: -52.516338524794456, R_score: -19.030612244897952, eL: 0.05, eR: 0.05\n",
      "episode: 135/500, time: 986, L_score: 169.47073445290582, R_score: 385.9836734693891, eL: 0.05, eR: 0.05\n",
      "episode: 136/500, time: 808, L_score: 119.75714285714265, R_score: 469.60739141233887, eL: 0.05, eR: 0.05\n",
      "episode: 137/500, time: 53, L_score: 6.320408163265303, R_score: 15.75522014464683, eL: 0.05, eR: 0.05\n",
      "episode: 138/500, time: 1574, L_score: 334.0465266664337, R_score: 616.8877551020527, eL: 0.05, eR: 0.05\n",
      "episode: 139/500, time: 52, L_score: 6.424489795918365, R_score: 16.005241027218386, eL: 0.05, eR: 0.05\n",
      "episode: 140/500, time: 238, L_score: 76.91229149848064, R_score: 65.5306122448981, eL: 0.05, eR: 0.05\n",
      "episode: 141/500, time: 1356, L_score: 385.6693877551009, R_score: 429.41544358393713, eL: 0.05, eR: 0.05\n",
      "episode: 142/500, time: 484, L_score: -26.37393825154824, R_score: 208.981632653062, eL: 0.05, eR: 0.05\n",
      "episode: 143/500, time: 1008, L_score: 36.444897959183585, R_score: 188.56476793468497, eL: 0.05, eR: 0.05\n",
      "episode: 144/500, time: 614, L_score: 21.031287908179944, R_score: 534.8183673469433, eL: 0.05, eR: 0.05\n",
      "episode: 145/500, time: 252, L_score: 88.25714285714227, R_score: 74.6102305609041, eL: 0.05, eR: 0.05\n",
      "episode: 146/500, time: 2969, L_score: 951.357006342501, R_score: 1292.1510204081746, eL: 0.05, eR: 0.05\n",
      "episode: 147/500, time: 52, L_score: 6.424489795918365, R_score: 15.981803247346123, eL: 0.05, eR: 0.05\n",
      "episode: 148/500, time: 621, L_score: 175.54489795918164, R_score: 126.53094528106949, eL: 0.05, eR: 0.05\n",
      "episode: 149/500, time: 204, L_score: 29.948979591836874, R_score: 44.92469121642638, eL: 0.05, eR: 0.05\n",
      "episode: 150/500, time: 90, L_score: 12.24897959183672, R_score: 23.902288069821367, eL: 0.05, eR: 0.05\n",
      "episode: 151/500, time: 537, L_score: -17.20868964411656, R_score: 185.7061224489782, eL: 0.05, eR: 0.05\n",
      "episode: 152/500, time: 175, L_score: 24.928571428571512, R_score: 34.19895486075077, eL: 0.05, eR: 0.05\n",
      "episode: 153/500, time: 293, L_score: 36.952063705689085, R_score: 381.5836734693913, eL: 0.05, eR: 0.05\n",
      "episode: 154/500, time: 86, L_score: 7.768623651888918, R_score: 17.375510204081618, eL: 0.05, eR: 0.05\n",
      "episode: 155/500, time: 643, L_score: 115.0136862805444, R_score: 215.15510204081585, eL: 0.05, eR: 0.05\n",
      "episode: 156/500, time: 439, L_score: 149.64807723539286, R_score: 98.85510204081554, eL: 0.05, eR: 0.05\n",
      "episode: 157/500, time: 208, L_score: 11.828048320588644, R_score: 8.981632653061336, eL: 0.05, eR: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 158/500, time: 607, L_score: 235.80408163264977, R_score: 354.97807392422106, eL: 0.05, eR: 0.05\n",
      "episode: 159/500, time: 1501, L_score: 203.05306122448897, R_score: 535.0104418265229, eL: 0.05, eR: 0.05\n",
      "episode: 160/500, time: 385, L_score: 143.1014461688056, R_score: 176.0306122448974, eL: 0.05, eR: 0.05\n",
      "episode: 161/500, time: 50, L_score: 5.408163265306121, R_score: 16.160342232785254, eL: 0.05, eR: 0.05\n",
      "episode: 162/500, time: 824, L_score: 169.33265306122243, R_score: 362.50909775590327, eL: 0.05, eR: 0.05\n",
      "episode: 163/500, time: 229, L_score: 27.33993183548123, R_score: 31.267346938775567, eL: 0.05, eR: 0.05\n",
      "episode: 164/500, time: 885, L_score: 20.040816326530376, R_score: 420.70489384516884, eL: 0.05, eR: 0.05\n",
      "episode: 165/500, time: 773, L_score: 248.68947739043534, R_score: 511.06326530612415, eL: 0.05, eR: 0.05\n",
      "episode: 166/500, time: 342, L_score: 43.116326530612554, R_score: 92.1827649260161, eL: 0.05, eR: 0.05\n",
      "episode: 167/500, time: 657, L_score: 138.7204081632648, R_score: 130.0887237041086, eL: 0.05, eR: 0.05\n",
      "episode: 168/500, time: 210, L_score: 24.632653061224563, R_score: 55.953443026584324, eL: 0.05, eR: 0.05\n",
      "episode: 169/500, time: 1394, L_score: 221.14489795917916, R_score: 388.760748035941, eL: 0.05, eR: 0.05\n",
      "episode: 170/500, time: 477, L_score: 32.15306122449027, R_score: 458.8233678778619, eL: 0.05, eR: 0.05\n",
      "episode: 171/500, time: 2159, L_score: 242.47346938774797, R_score: 836.1466647768765, eL: 0.05, eR: 0.05\n",
      "episode: 172/500, time: 53, L_score: -2.767239996262589, R_score: 11.422448979591834, eL: 0.05, eR: 0.05\n",
      "episode: 173/500, time: 377, L_score: 159.0775510204073, R_score: 50.07778486311665, eL: 0.05, eR: 0.05\n",
      "episode: 174/500, time: 156, L_score: 71.53265306122422, R_score: 19.458205916566882, eL: 0.05, eR: 0.05\n",
      "episode: 175/500, time: 1326, L_score: 323.09306663068725, R_score: 304.7306122448982, eL: 0.05, eR: 0.05\n",
      "episode: 176/500, time: 1554, L_score: 196.13117487646113, R_score: 249.2918367346917, eL: 0.05, eR: 0.05\n",
      "episode: 177/500, time: 642, L_score: 72.9683506420873, R_score: 331.53469387755337, eL: 0.05, eR: 0.05\n",
      "episode: 178/500, time: 431, L_score: 101.91545281659944, R_score: 427.5428571428617, eL: 0.05, eR: 0.05\n",
      "episode: 179/500, time: 526, L_score: 44.55714285714319, R_score: 99.71404166994643, eL: 0.05, eR: 0.05\n",
      "episode: 180/500, time: 456, L_score: 113.84081632652963, R_score: 223.2419641110746, eL: 0.05, eR: 0.05\n",
      "episode: 181/500, time: 697, L_score: 101.84489795918378, R_score: 159.9978540888632, eL: 0.05, eR: 0.05\n",
      "episode: 182/500, time: 392, L_score: 57.05714285714324, R_score: 283.70481792257243, eL: 0.05, eR: 0.05\n",
      "episode: 183/500, time: 431, L_score: 112.85306122448947, R_score: 78.97451164738628, eL: 0.05, eR: 0.05\n",
      "episode: 184/500, time: 423, L_score: 52.53673469387785, R_score: 146.86183100881107, eL: 0.05, eR: 0.05\n",
      "episode: 185/500, time: 171, L_score: 23.71224489795926, R_score: 214.85465899889653, eL: 0.05, eR: 0.05\n",
      "episode: 186/500, time: 579, L_score: 74.49248444983216, R_score: 129.63877551020335, eL: 0.05, eR: 0.05\n",
      "episode: 187/500, time: 813, L_score: 207.62857142856802, R_score: 172.91006973960677, eL: 0.05, eR: 0.05\n",
      "episode: 188/500, time: 164, L_score: 23.84081632653067, R_score: -4.26073449404492, eL: 0.05, eR: 0.05\n",
      "episode: 189/500, time: 1766, L_score: 417.55714285714066, R_score: 863.0038598356234, eL: 0.05, eR: 0.05\n",
      "episode: 190/500, time: 168, L_score: 26.073469387755168, R_score: 5.27006461199823, eL: 0.05, eR: 0.05\n",
      "episode: 191/500, time: 164, L_score: 25.57346938775516, R_score: 0.7855901830748735, eL: 0.05, eR: 0.05\n",
      "episode: 192/500, time: 256, L_score: 87.34897959183614, R_score: 76.35780207026704, eL: 0.05, eR: 0.05\n",
      "episode: 193/500, time: 261, L_score: 22.44872774576359, R_score: 63.44285714285736, eL: 0.05, eR: 0.05\n",
      "episode: 194/500, time: 282, L_score: 83.74081632652992, R_score: 64.61856759228777, eL: 0.05, eR: 0.05\n",
      "episode: 195/500, time: 1094, L_score: 315.0653061224462, R_score: 359.6916267575082, eL: 0.05, eR: 0.05\n",
      "episode: 196/500, time: 161, L_score: 74.73673469387703, R_score: 41.77006193196952, eL: 0.05, eR: 0.05\n",
      "episode: 197/500, time: 339, L_score: 100.353061224489, R_score: 84.57787275003588, eL: 0.05, eR: 0.05\n",
      "episode: 198/500, time: 883, L_score: 141.38163265305724, R_score: 327.5589376606434, eL: 0.05, eR: 0.05\n",
      "episode: 199/500, time: 523, L_score: 112.15306122448915, R_score: 234.240492552603, eL: 0.05, eR: 0.05\n",
      "episode: 200/500, time: 316, L_score: -42.80612244897938, R_score: 83.04083826577437, eL: 0.05, eR: 0.05\n",
      "episode: 201/500, time: 408, L_score: 8.692437308727113, R_score: 49.577551020407775, eL: 0.05, eR: 0.05\n",
      "episode: 202/500, time: 566, L_score: 123.13673469387759, R_score: 140.54659746240748, eL: 0.05, eR: 0.05\n",
      "episode: 203/500, time: 245, L_score: 35.25306122448999, R_score: 58.20437107258328, eL: 0.05, eR: 0.05\n",
      "episode: 204/500, time: 555, L_score: 120.60580201225682, R_score: 330.01836734694314, eL: 0.05, eR: 0.05\n",
      "episode: 205/500, time: 1430, L_score: 204.14606289998744, R_score: 820.8795918367443, eL: 0.05, eR: 0.05\n",
      "episode: 206/500, time: 519, L_score: 37.257142857143116, R_score: 61.30942274346707, eL: 0.05, eR: 0.05\n",
      "episode: 207/500, time: 762, L_score: 311.1571428571422, R_score: 360.2304068375278, eL: 0.05, eR: 0.05\n",
      "episode: 208/500, time: 298, L_score: 141.81632653061163, R_score: 278.2950484361616, eL: 0.05, eR: 0.05\n",
      "episode: 209/500, time: 1209, L_score: 199.13648346904566, R_score: 294.43469387755084, eL: 0.05, eR: 0.05\n",
      "episode: 210/500, time: 282, L_score: 90.04081632652984, R_score: 173.77293093839862, eL: 0.05, eR: 0.05\n",
      "episode: 211/500, time: 52, L_score: 6.832653061224487, R_score: 15.865034097412211, eL: 0.05, eR: 0.05\n",
      "episode: 212/500, time: 50, L_score: 6.224489795918366, R_score: 16.13894841470333, eL: 0.05, eR: 0.05\n",
      "episode: 213/500, time: 455, L_score: 114.23673469387667, R_score: 135.35948867140323, eL: 0.05, eR: 0.05\n",
      "episode: 214/500, time: 380, L_score: 99.85714285714207, R_score: 414.3642132845831, eL: 0.05, eR: 0.05\n",
      "episode: 215/500, time: 172, L_score: 13.276834161369653, R_score: 24.9551020408163, eL: 0.05, eR: 0.05\n",
      "episode: 216/500, time: 246, L_score: 34.33265306122468, R_score: 61.32734175178029, eL: 0.05, eR: 0.05\n",
      "episode: 217/500, time: 672, L_score: 139.93265306122458, R_score: 169.66199901565048, eL: 0.05, eR: 0.05\n",
      "episode: 218/500, time: 477, L_score: 168.94489795918273, R_score: 234.83566103918082, eL: 0.05, eR: 0.05\n",
      "episode: 219/500, time: 213, L_score: 31.34489795918378, R_score: 36.834632134101525, eL: 0.05, eR: 0.05\n",
      "episode: 220/500, time: 964, L_score: 238.1734693877508, R_score: 244.51032903375736, eL: 0.05, eR: 0.05\n",
      "episode: 221/500, time: 628, L_score: 92.12040816326464, R_score: 117.30002931170968, eL: 0.05, eR: 0.05\n",
      "episode: 222/500, time: 1052, L_score: -68.23465763677277, R_score: 690.1897959183765, eL: 0.05, eR: 0.05\n",
      "episode: 223/500, time: 962, L_score: 206.54897959183242, R_score: 362.5928284121244, eL: 0.05, eR: 0.05\n",
      "episode: 224/500, time: 52, L_score: -1.1235617869754613, R_score: 11.730612244897957, eL: 0.05, eR: 0.05\n",
      "episode: 225/500, time: 452, L_score: 167.3489795918359, R_score: 420.5211839558314, eL: 0.05, eR: 0.05\n",
      "episode: 226/500, time: 1155, L_score: 306.95714285714314, R_score: 272.9409399103509, eL: 0.05, eR: 0.05\n",
      "episode: 227/500, time: 169, L_score: 25.24489795918375, R_score: 30.228259350867763, eL: 0.05, eR: 0.05\n",
      "episode: 228/500, time: 621, L_score: 45.65714285714324, R_score: 114.06924856500387, eL: 0.05, eR: 0.05\n",
      "episode: 229/500, time: 1008, L_score: 238.69097428800902, R_score: 482.4959183673522, eL: 0.05, eR: 0.05\n",
      "episode: 230/500, time: 717, L_score: 154.66122448979462, R_score: 186.50303909707253, eL: 0.05, eR: 0.05\n",
      "episode: 231/500, time: 53, L_score: 6.728571428571426, R_score: 16.517078544642338, eL: 0.05, eR: 0.05\n",
      "episode: 232/500, time: 319, L_score: -165.5265306122457, R_score: -20.297634356852402, eL: 0.05, eR: 0.05\n",
      "episode: 233/500, time: 411, L_score: 22.24081632653066, R_score: 326.6103764104943, eL: 0.05, eR: 0.05\n",
      "episode: 234/500, time: 837, L_score: 122.76122448979567, R_score: 176.31683861430693, eL: 0.05, eR: 0.05\n",
      "episode: 235/500, time: 575, L_score: 183.43673469387696, R_score: 341.3225529926914, eL: 0.05, eR: 0.05\n",
      "episode: 236/500, time: 2492, L_score: 552.3751679157988, R_score: 1377.7551020407946, eL: 0.05, eR: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 237/500, time: 1031, L_score: 131.36122448979603, R_score: 519.3699364760247, eL: 0.05, eR: 0.05\n",
      "episode: 238/500, time: 455, L_score: 116.16122448979505, R_score: 136.24072337652217, eL: 0.05, eR: 0.05\n",
      "episode: 239/500, time: 671, L_score: 97.64489795918377, R_score: 142.588653393814, eL: 0.05, eR: 0.05\n",
      "episode: 240/500, time: 878, L_score: 374.58979591836726, R_score: 299.48846547306226, eL: 0.05, eR: 0.05\n",
      "episode: 241/500, time: 50, L_score: 5.816326530612243, R_score: 17.002374704880605, eL: 0.05, eR: 0.05\n",
      "episode: 242/500, time: 690, L_score: 190.24081632652724, R_score: 168.29847139662823, eL: 0.05, eR: 0.05\n",
      "episode: 243/500, time: 1731, L_score: 111.61051546548171, R_score: 543.5224489795992, eL: 0.05, eR: 0.05\n",
      "episode: 244/500, time: 237, L_score: 31.161224489795988, R_score: 146.34315185102412, eL: 0.05, eR: 0.05\n",
      "episode: 245/500, time: 292, L_score: 36.95714285714297, R_score: 61.21950616353513, eL: 0.05, eR: 0.05\n",
      "episode: 246/500, time: 384, L_score: 31.753061224489866, R_score: 88.74893142179285, eL: 0.05, eR: 0.05\n",
      "episode: 247/500, time: 434, L_score: 111.0244897959174, R_score: 84.87940350860522, eL: 0.05, eR: 0.05\n",
      "episode: 248/500, time: 1555, L_score: 372.54489795918374, R_score: 512.6905620134487, eL: 0.05, eR: 0.05\n",
      "episode: 249/500, time: 2390, L_score: 514.0858542073231, R_score: 1213.467346938774, eL: 0.05, eR: 0.05\n",
      "episode: 250/500, time: 391, L_score: -183.5428571428578, R_score: 108.19013501768607, eL: 0.05, eR: 0.05\n",
      "episode: 251/500, time: 168, L_score: 24.44081632653068, R_score: 8.25217022727955, eL: 0.05, eR: 0.05\n",
      "episode: 252/500, time: 450, L_score: 110.47346938775438, R_score: 116.41941297404378, eL: 0.05, eR: 0.05\n",
      "episode: 253/500, time: 1047, L_score: 338.08887669866766, R_score: 447.7183673469425, eL: 0.05, eR: 0.05\n",
      "episode: 254/500, time: 319, L_score: 47.044897959183764, R_score: 42.15063896114325, eL: 0.05, eR: 0.05\n",
      "episode: 255/500, time: 1698, L_score: 542.0489795918429, R_score: 577.9349617383266, eL: 0.05, eR: 0.05\n",
      "episode: 256/500, time: 77, L_score: 4.266847224492789, R_score: 13.618367346938765, eL: 0.05, eR: 0.05\n",
      "episode: 257/500, time: 83, L_score: 10.521482133174182, R_score: 15.034693877551007, eL: 0.05, eR: 0.05\n",
      "episode: 258/500, time: 52, L_score: 6.016326530612242, R_score: 16.036448688888818, eL: 0.05, eR: 0.05\n",
      "episode: 259/500, time: 374, L_score: 55.240816326530954, R_score: 229.76161365908274, eL: 0.05, eR: 0.05\n",
      "episode: 260/500, time: 1438, L_score: 383.2571428571453, R_score: 532.4543131819842, eL: 0.05, eR: 0.05\n",
      "episode: 261/500, time: 589, L_score: 141.97346938775513, R_score: 293.3172892525819, eL: 0.05, eR: 0.05\n",
      "episode: 262/500, time: 79, L_score: 5.257918630697326, R_score: 14.226530612244886, eL: 0.05, eR: 0.05\n",
      "episode: 263/500, time: 153, L_score: 7.184588951153955, R_score: 22.851020408163226, eL: 0.05, eR: 0.05\n",
      "episode: 264/500, time: 564, L_score: 117.15714285714269, R_score: 407.6759758989172, eL: 0.05, eR: 0.05\n",
      "episode: 265/500, time: 53, L_score: -1.474887074968473, R_score: 11.01428571428571, eL: 0.05, eR: 0.05\n",
      "episode: 266/500, time: 1098, L_score: 146.5408163265295, R_score: 437.32213954686165, eL: 0.05, eR: 0.05\n",
      "episode: 267/500, time: 883, L_score: 171.35306122448904, R_score: 236.55325781270082, eL: 0.05, eR: 0.05\n",
      "episode: 268/500, time: 490, L_score: -14.938775510203712, R_score: -60.862653470910644, eL: 0.05, eR: 0.05\n",
      "episode: 269/500, time: 343, L_score: 88.15973933646622, R_score: 190.49387755102003, eL: 0.05, eR: 0.05\n",
      "episode: 270/500, time: 1835, L_score: 609.9685085799774, R_score: 790.9938775510307, eL: 0.05, eR: 0.05\n",
      "episode: 271/500, time: 572, L_score: 31.12857142857162, R_score: 166.14301861588032, eL: 0.05, eR: 0.05\n",
      "episode: 272/500, time: 338, L_score: 48.72448979591862, R_score: 83.66287364128002, eL: 0.05, eR: 0.05\n",
      "episode: 273/500, time: 276, L_score: 42.02443208996145, R_score: 44.367346938775555, eL: 0.05, eR: 0.05\n",
      "episode: 274/500, time: 920, L_score: 241.3571428571399, R_score: 184.73852079012372, eL: 0.05, eR: 0.05\n",
      "episode: 275/500, time: 1420, L_score: 335.85736673008824, R_score: 926.3918367347046, eL: 0.05, eR: 0.05\n",
      "episode: 276/500, time: 996, L_score: 319.2283609744393, R_score: 332.09795918367445, eL: 0.05, eR: 0.05\n",
      "episode: 277/500, time: 699, L_score: 170.6771282655284, R_score: 154.7979591836718, eL: 0.05, eR: 0.05\n",
      "episode: 278/500, time: 734, L_score: 14.061224489796269, R_score: 221.60638840703876, eL: 0.05, eR: 0.05\n",
      "episode: 279/500, time: 355, L_score: -31.530612244897853, R_score: 310.5944795230226, eL: 0.05, eR: 0.05\n",
      "episode: 280/500, time: 386, L_score: 56.43265306122475, R_score: 97.21283210985833, eL: 0.05, eR: 0.05\n",
      "episode: 281/500, time: 623, L_score: 224.7905696158807, R_score: 245.55918367346797, eL: 0.05, eR: 0.05\n",
      "episode: 282/500, time: 520, L_score: 176.065306122448, R_score: 299.13650367532955, eL: 0.05, eR: 0.05\n",
      "episode: 283/500, time: 804, L_score: -45.81886428813631, R_score: 385.0979591836774, eL: 0.05, eR: 0.05\n",
      "episode: 284/500, time: 1923, L_score: 179.13265306121528, R_score: 689.1931490244398, eL: 0.05, eR: 0.05\n",
      "episode: 285/500, time: 726, L_score: 108.45306122448856, R_score: 179.10210553586458, eL: 0.05, eR: 0.05\n",
      "episode: 286/500, time: 1474, L_score: 223.4653061224434, R_score: 370.64895133248297, eL: 0.05, eR: 0.05\n",
      "episode: 287/500, time: 866, L_score: 279.3571428571396, R_score: 339.52566374813864, eL: 0.05, eR: 0.05\n",
      "episode: 288/500, time: 169, L_score: 25.961224489795992, R_score: 30.08574624137752, eL: 0.05, eR: 0.05\n",
      "episode: 289/500, time: 1276, L_score: 332.42448979591865, R_score: 640.9359571049329, eL: 0.05, eR: 0.05\n",
      "episode: 290/500, time: 512, L_score: 121.7408163265304, R_score: 181.22182537807495, eL: 0.05, eR: 0.05\n",
      "episode: 291/500, time: 328, L_score: 40.64897959183688, R_score: 77.66314938764747, eL: 0.05, eR: 0.05\n",
      "episode: 292/500, time: 825, L_score: 140.24489795918342, R_score: 435.4996344372449, eL: 0.05, eR: 0.05\n",
      "episode: 293/500, time: 1157, L_score: 241.88571428571143, R_score: 446.76009339595674, eL: 0.05, eR: 0.05\n",
      "episode: 294/500, time: 240, L_score: 76.48272043196485, R_score: 61.75510204081645, eL: 0.05, eR: 0.05\n",
      "episode: 295/500, time: 2657, L_score: 889.6589185005936, R_score: 847.1918367347153, eL: 0.05, eR: 0.05\n",
      "episode: 296/500, time: 631, L_score: 131.6367346938778, R_score: 106.40964479495979, eL: 0.05, eR: 0.05\n",
      "episode: 297/500, time: 765, L_score: 211.9367346938775, R_score: 601.4270944199104, eL: 0.05, eR: 0.05\n",
      "episode: 298/500, time: 2542, L_score: 328.997959183675, R_score: 902.4262622228435, eL: 0.05, eR: 0.05\n",
      "episode: 299/500, time: 1773, L_score: 339.4530612244866, R_score: 517.0922594908552, eL: 0.05, eR: 0.05\n",
      "episode: 300/500, time: 50, L_score: 5.408163265306121, R_score: 16.246733559662864, eL: 0.05, eR: 0.05\n",
      "episode: 301/500, time: 628, L_score: 32.75714285714294, R_score: 267.52131131707347, eL: 0.05, eR: 0.05\n",
      "episode: 302/500, time: 389, L_score: 108.96938775510154, R_score: 267.1598719126972, eL: 0.05, eR: 0.05\n",
      "episode: 303/500, time: 668, L_score: 234.0026581943899, R_score: 250.03469387754996, eL: 0.05, eR: 0.05\n",
      "episode: 304/500, time: 220, L_score: 25.8489795918368, R_score: 27.462602421703718, eL: 0.05, eR: 0.05\n",
      "episode: 305/500, time: 446, L_score: 135.83265306122334, R_score: 126.46409060075638, eL: 0.05, eR: 0.05\n",
      "episode: 306/500, time: 835, L_score: 110.92857142857186, R_score: 189.2269401385629, eL: 0.05, eR: 0.05\n",
      "episode: 307/500, time: 336, L_score: 100.04897959183633, R_score: 102.43636775378306, eL: 0.05, eR: 0.05\n",
      "episode: 308/500, time: 291, L_score: 93.72857142857083, R_score: 70.95481338548637, eL: 0.05, eR: 0.05\n",
      "episode: 309/500, time: 2072, L_score: 176.26803186732457, R_score: 482.9265306122473, eL: 0.05, eR: 0.05\n",
      "episode: 310/500, time: 2154, L_score: 491.56122448979437, R_score: 719.8935038512388, eL: 0.05, eR: 0.05\n",
      "episode: 311/500, time: 1746, L_score: 383.56530612245047, R_score: 920.6420960594971, eL: 0.05, eR: 0.05\n",
      "episode: 312/500, time: 646, L_score: 186.03265306122205, R_score: 277.2403829835963, eL: 0.05, eR: 0.05\n",
      "episode: 313/500, time: 813, L_score: 164.35306122448966, R_score: 736.201215151512, eL: 0.05, eR: 0.05\n",
      "episode: 314/500, time: 78, L_score: 5.9124800778280235, R_score: 14.126530612244887, eL: 0.05, eR: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 315/500, time: 800, L_score: 99.25356012542811, R_score: 492.9510204081671, eL: 0.05, eR: 0.05\n",
      "episode: 316/500, time: 82, L_score: 7.9059723014872265, R_score: 15.955102040816314, eL: 0.05, eR: 0.05\n",
      "episode: 317/500, time: 1457, L_score: 164.36485548955162, R_score: 516.8306122449086, eL: 0.05, eR: 0.05\n",
      "episode: 318/500, time: 792, L_score: 42.54897959183705, R_score: 280.88494831874596, eL: 0.05, eR: 0.05\n",
      "episode: 319/500, time: 2554, L_score: 480.53265306123023, R_score: 690.218771884032, eL: 0.05, eR: 0.05\n",
      "episode: 320/500, time: 1230, L_score: 333.15714285714364, R_score: 562.1920926139474, eL: 0.05, eR: 0.05\n",
      "episode: 321/500, time: 811, L_score: 147.34081632653084, R_score: 143.70668719126846, eL: 0.05, eR: 0.05\n",
      "episode: 322/500, time: 839, L_score: 213.5448979591798, R_score: 273.8591783639131, eL: 0.05, eR: 0.05\n",
      "episode: 323/500, time: 344, L_score: 45.44897959183683, R_score: 46.26681587281038, eL: 0.05, eR: 0.05\n",
      "episode: 324/500, time: 364, L_score: 149.47956618062153, R_score: 83.53061224489721, eL: 0.05, eR: 0.05\n",
      "episode: 325/500, time: 2984, L_score: 351.8938775510154, R_score: 888.8881191033256, eL: 0.05, eR: 0.05\n",
      "episode: 326/500, time: 150, L_score: 9.073072076048081, R_score: 22.142857142857107, eL: 0.05, eR: 0.05\n",
      "episode: 327/500, time: 579, L_score: 78.23673469387731, R_score: 136.98767165520977, eL: 0.05, eR: 0.05\n",
      "episode: 328/500, time: 664, L_score: 183.8020519355578, R_score: 318.2469387755117, eL: 0.05, eR: 0.05\n",
      "episode: 329/500, time: 1277, L_score: 276.2857142857121, R_score: 807.7191286191421, eL: 0.05, eR: 0.05\n",
      "episode: 330/500, time: 565, L_score: 127.26122448979592, R_score: 140.48387451902389, eL: 0.05, eR: 0.05\n",
      "episode: 331/500, time: 85, L_score: 13.17755102040815, R_score: 22.190043891274936, eL: 0.05, eR: 0.05\n",
      "episode: 332/500, time: 969, L_score: 136.55306122448943, R_score: 277.5007193800404, eL: 0.05, eR: 0.05\n",
      "episode: 333/500, time: 1438, L_score: 313.98163265305897, R_score: 378.8384251618662, eL: 0.05, eR: 0.05\n",
      "episode: 334/500, time: 563, L_score: -29.53061224489811, R_score: 33.02205969054383, eL: 0.05, eR: 0.05\n",
      "episode: 335/500, time: 470, L_score: 120.92448979591836, R_score: 212.72600444128685, eL: 0.05, eR: 0.05\n",
      "episode: 336/500, time: 169, L_score: 25.16122448979598, R_score: 24.657216480126543, eL: 0.05, eR: 0.05\n",
      "episode: 337/500, time: 227, L_score: 24.5122448979592, R_score: 584.6817422665009, eL: 0.05, eR: 0.05\n",
      "episode: 338/500, time: 1976, L_score: 487.0489795918411, R_score: 696.8992933053984, eL: 0.05, eR: 0.05\n",
      "episode: 339/500, time: 1178, L_score: 316.14897959183486, R_score: 567.7423943384277, eL: 0.05, eR: 0.05\n",
      "episode: 340/500, time: 879, L_score: 231.1938775510202, R_score: 877.786005423785, eL: 0.05, eR: 0.05\n",
      "episode: 341/500, time: 958, L_score: 179.18163265305967, R_score: 260.21114910349525, eL: 0.05, eR: 0.05\n",
      "episode: 342/500, time: 620, L_score: -53.33469387755112, R_score: 49.90530954075942, eL: 0.05, eR: 0.05\n",
      "episode: 343/500, time: 300, L_score: 25.089795918367322, R_score: 146.4468885350858, eL: 0.05, eR: 0.05\n",
      "episode: 344/500, time: 607, L_score: 169.32405277363182, R_score: 351.45918367347156, eL: 0.05, eR: 0.05\n",
      "episode: 345/500, time: 50, L_score: 5.816326530612243, R_score: 17.614740656180867, eL: 0.05, eR: 0.05\n",
      "episode: 346/500, time: 100, L_score: 12.040816326530592, R_score: 26.955509688901998, eL: 0.05, eR: 0.05\n",
      "episode: 347/500, time: 51, L_score: 6.120408163265304, R_score: 16.094524395633094, eL: 0.05, eR: 0.05\n",
      "episode: 348/500, time: 1978, L_score: 520.8244897959213, R_score: 457.15145590917507, eL: 0.05, eR: 0.05\n",
      "episode: 349/500, time: 1186, L_score: 266.04897959183654, R_score: 575.4803499168908, eL: 0.05, eR: 0.05\n",
      "episode: 350/500, time: 238, L_score: 77.84473588227067, R_score: 59.32244897959196, eL: 0.05, eR: 0.05\n",
      "episode: 351/500, time: 165, L_score: 24.453061224489854, R_score: 4.665468719112694, eL: 0.05, eR: 0.05\n",
      "episode: 352/500, time: 667, L_score: 89.65306122448979, R_score: 144.75915381161863, eL: 0.05, eR: 0.05\n",
      "episode: 353/500, time: 758, L_score: 108.34776943660674, R_score: 147.63469387754998, eL: 0.05, eR: 0.05\n",
      "episode: 354/500, time: 497, L_score: 112.72857142857124, R_score: 131.51050614533753, eL: 0.05, eR: 0.05\n",
      "episode: 355/500, time: 1740, L_score: 585.968483087757, R_score: 535.6714285714347, eL: 0.05, eR: 0.05\n",
      "episode: 356/500, time: 1058, L_score: 296.7494496705085, R_score: 328.1306122449021, eL: 0.05, eR: 0.05\n",
      "episode: 357/500, time: 231, L_score: 24.504081632653072, R_score: 84.9542536442165, eL: 0.05, eR: 0.05\n",
      "episode: 358/500, time: 973, L_score: -11.779591836733662, R_score: 286.1315616730768, eL: 0.05, eR: 0.05\n",
      "episode: 359/500, time: 1708, L_score: 445.54897959183944, R_score: 563.5545490311625, eL: 0.05, eR: 0.05\n",
      "episode: 360/500, time: 897, L_score: 272.0986396528984, R_score: 200.25510204081502, eL: 0.05, eR: 0.05\n",
      "episode: 361/500, time: 1013, L_score: 244.0448979591817, R_score: 574.1018371478006, eL: 0.05, eR: 0.05\n",
      "episode: 362/500, time: 762, L_score: 110.6886217032445, R_score: 92.5346938775511, eL: 0.05, eR: 0.05\n",
      "episode: 363/500, time: 50, L_score: 7.5489795918367335, R_score: 117.25825513717282, eL: 0.05, eR: 0.05\n",
      "episode: 364/500, time: 1530, L_score: 374.49986320188304, R_score: 498.4510204081695, eL: 0.05, eR: 0.05\n",
      "episode: 365/500, time: 1159, L_score: 252.91739114699652, R_score: 229.93469387754925, eL: 0.05, eR: 0.05\n",
      "episode: 366/500, time: 2258, L_score: 433.1285714285755, R_score: 979.261411731813, eL: 0.05, eR: 0.05\n",
      "episode: 367/500, time: 78, L_score: 10.248979591836724, R_score: 21.777232122153485, eL: 0.05, eR: 0.05\n",
      "episode: 368/500, time: 327, L_score: 98.34489795918326, R_score: 168.9520243982547, eL: 0.05, eR: 0.05\n",
      "episode: 369/500, time: 1940, L_score: 408.8612244898003, R_score: 477.46041777457157, eL: 0.05, eR: 0.05\n",
      "episode: 370/500, time: 1071, L_score: 51.36530612244917, R_score: 515.6257303225003, eL: 0.05, eR: 0.05\n",
      "episode: 371/500, time: 461, L_score: 65.25306122449042, R_score: 122.79873894692416, eL: 0.05, eR: 0.05\n",
      "episode: 372/500, time: 1515, L_score: 351.761224489794, R_score: 654.1800404753125, eL: 0.05, eR: 0.05\n",
      "episode: 373/500, time: 341, L_score: 136.82040816326474, R_score: 83.3120167362063, eL: 0.05, eR: 0.05\n",
      "episode: 374/500, time: 103, L_score: 11.728571428571408, R_score: 30.290475783872186, eL: 0.05, eR: 0.05\n",
      "episode: 375/500, time: 50, L_score: 5.816326530612243, R_score: 16.785371373584304, eL: 0.05, eR: 0.05\n",
      "episode: 376/500, time: 208, L_score: 129.0326530612237, R_score: 50.24774916399102, eL: 0.05, eR: 0.05\n",
      "episode: 377/500, time: 366, L_score: 89.18809507543375, R_score: 199.02244897959133, eL: 0.05, eR: 0.05\n",
      "episode: 378/500, time: 724, L_score: 251.23923957650257, R_score: 470.6265306122495, eL: 0.05, eR: 0.05\n",
      "episode: 379/500, time: 158, L_score: 98.43762381524446, R_score: 23.146938775510165, eL: 0.05, eR: 0.05\n",
      "episode: 380/500, time: 379, L_score: 105.42857142857116, R_score: 77.07524372746043, eL: 0.05, eR: 0.05\n",
      "episode: 381/500, time: 750, L_score: 77.92448979591828, R_score: 418.20391797274976, eL: 0.05, eR: 0.05\n",
      "episode: 382/500, time: 83, L_score: 4.8822440761571, R_score: 16.055102040816312, eL: 0.05, eR: 0.05\n",
      "episode: 383/500, time: 432, L_score: 153.81939832424897, R_score: 171.63469387755055, eL: 0.05, eR: 0.05\n",
      "episode: 384/500, time: 282, L_score: 42.04897959183692, R_score: 59.359211042384835, eL: 0.05, eR: 0.05\n",
      "episode: 385/500, time: 134, L_score: 69.5408163265305, R_score: 40.14812602181487, eL: 0.05, eR: 0.05\n",
      "episode: 386/500, time: 50, L_score: 5.816326530612243, R_score: 16.87335932350708, eL: 0.05, eR: 0.05\n",
      "episode: 387/500, time: 94, L_score: -0.8533542953653459, R_score: 18.991836734693862, eL: 0.05, eR: 0.05\n",
      "episode: 388/500, time: 415, L_score: 46.15083555174178, R_score: 307.88367346939054, eL: 0.05, eR: 0.05\n",
      "episode: 389/500, time: 156, L_score: 9.499359999754653, R_score: 23.96734693877547, eL: 0.05, eR: 0.05\n",
      "episode: 390/500, time: 961, L_score: 221.25657598488976, R_score: 528.5755102040885, eL: 0.05, eR: 0.05\n",
      "episode: 391/500, time: 328, L_score: 47.23265306122459, R_score: 77.75198088624153, eL: 0.05, eR: 0.05\n",
      "episode: 392/500, time: 229, L_score: 13.609582244351273, R_score: 30.45102040816332, eL: 0.05, eR: 0.05\n",
      "episode: 393/500, time: 50, L_score: 6.224489795918366, R_score: 16.154630771007675, eL: 0.05, eR: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 394/500, time: 383, L_score: 151.92857142857082, R_score: 95.52782152796719, eL: 0.05, eR: 0.05\n",
      "episode: 395/500, time: 399, L_score: 100.38406998635604, R_score: 101.85102040816284, eL: 0.05, eR: 0.05\n",
      "episode: 396/500, time: 771, L_score: 244.14489795917976, R_score: 179.8190488280783, eL: 0.05, eR: 0.05\n",
      "episode: 397/500, time: 161, L_score: 14.320408163265276, R_score: 54.32891158729882, eL: 0.05, eR: 0.05\n",
      "episode: 398/500, time: 809, L_score: 262.90255774904097, R_score: 384.03061224490295, eL: 0.05, eR: 0.05\n",
      "episode: 399/500, time: 1591, L_score: 331.76122448979504, R_score: 571.8202195625638, eL: 0.05, eR: 0.05\n",
      "episode: 400/500, time: 656, L_score: -158.23877551020752, R_score: 87.60234009335854, eL: 0.05, eR: 0.05\n",
      "episode: 401/500, time: 441, L_score: 108.27088074503946, R_score: 189.93469387754993, eL: 0.05, eR: 0.05\n",
      "episode: 402/500, time: 1758, L_score: 347.032653061227, R_score: 641.337457406152, eL: 0.05, eR: 0.05\n",
      "episode: 403/500, time: 50, L_score: 6.224489795918366, R_score: 15.924929360966086, eL: 0.05, eR: 0.05\n",
      "episode: 404/500, time: 88, L_score: 12.765306122448965, R_score: 23.567654214346845, eL: 0.05, eR: 0.05\n",
      "episode: 405/500, time: 825, L_score: 147.01567398843858, R_score: 187.83061224489597, eL: 0.05, eR: 0.05\n",
      "episode: 406/500, time: 907, L_score: 286.9612244897952, R_score: 415.29153906948125, eL: 0.05, eR: 0.05\n",
      "episode: 407/500, time: 486, L_score: 119.3571428571428, R_score: 289.36957696294206, eL: 0.05, eR: 0.05\n",
      "episode: 408/500, time: 1218, L_score: 113.73673469387678, R_score: 374.7826545602901, eL: 0.05, eR: 0.05\n",
      "episode: 409/500, time: 868, L_score: 69.45714285714392, R_score: 1004.6014104023564, eL: 0.05, eR: 0.05\n",
      "episode: 410/500, time: 89, L_score: 12.761224489795904, R_score: 23.84692282702402, eL: 0.05, eR: 0.05\n",
      "episode: 411/500, time: 92, L_score: 11.732653061224473, R_score: 23.81373702041519, eL: 0.05, eR: 0.05\n",
      "episode: 412/500, time: 240, L_score: 30.637844849883955, R_score: 33.387755102040884, eL: 0.05, eR: 0.05\n",
      "episode: 413/500, time: 738, L_score: 159.34897959183633, R_score: 323.16823181165506, eL: 0.05, eR: 0.05\n",
      "episode: 414/500, time: 166, L_score: 57.6377678583103, R_score: 25.987755102040783, eL: 0.05, eR: 0.05\n",
      "episode: 415/500, time: 305, L_score: 145.12857142857084, R_score: 75.55865959926314, eL: 0.05, eR: 0.05\n",
      "episode: 416/500, time: 209, L_score: 28.520408163265444, R_score: 151.76080441771873, eL: 0.05, eR: 0.05\n",
      "episode: 417/500, time: 695, L_score: 43.94850613964399, R_score: 492.34693877551393, eL: 0.05, eR: 0.05\n",
      "episode: 418/500, time: 1830, L_score: 511.8734693877587, R_score: 1135.58323461291, eL: 0.05, eR: 0.05\n",
      "episode: 419/500, time: 302, L_score: 43.932653061224556, R_score: 57.07036917754765, eL: 0.05, eR: 0.05\n",
      "episode: 420/500, time: 1068, L_score: 131.56938775510105, R_score: 437.7717426015619, eL: 0.05, eR: 0.05\n",
      "episode: 421/500, time: 800, L_score: 181.45714285714192, R_score: 263.6056285403146, eL: 0.05, eR: 0.05\n",
      "episode: 422/500, time: 1783, L_score: 483.26284073312814, R_score: 754.9979591836815, eL: 0.05, eR: 0.05\n",
      "episode: 423/500, time: 216, L_score: 28.84897959183686, R_score: 45.0016071250064, eL: 0.05, eR: 0.05\n",
      "episode: 424/500, time: 453, L_score: 66.34489795918421, R_score: 103.4549691419838, eL: 0.05, eR: 0.05\n",
      "episode: 425/500, time: 1076, L_score: 191.4326530612234, R_score: 320.26598100420154, eL: 0.05, eR: 0.05\n",
      "episode: 426/500, time: 52, L_score: 5.60816326530612, R_score: 15.664949484175041, eL: 0.05, eR: 0.05\n",
      "episode: 427/500, time: 465, L_score: 164.58571428571344, R_score: 225.15618061529617, eL: 0.05, eR: 0.05\n",
      "episode: 428/500, time: 510, L_score: 222.57346938775248, R_score: 340.18530866474447, eL: 0.05, eR: 0.05\n",
      "episode: 429/500, time: 375, L_score: 98.24897959183603, R_score: 198.5404508412409, eL: 0.05, eR: 0.05\n",
      "episode: 430/500, time: 1307, L_score: 394.22040816326216, R_score: 457.88722680369204, eL: 0.05, eR: 0.05\n",
      "episode: 431/500, time: 1156, L_score: 301.3176014168504, R_score: 420.0551020408191, eL: 0.05, eR: 0.05\n",
      "episode: 432/500, time: 261, L_score: 21.621300037656248, R_score: 68.82653061224508, eL: 0.05, eR: 0.05\n",
      "episode: 433/500, time: 281, L_score: 88.52857142857064, R_score: 271.07321420329464, eL: 0.05, eR: 0.05\n",
      "episode: 434/500, time: 798, L_score: 112.22448979591783, R_score: 186.70611546617158, eL: 0.05, eR: 0.05\n",
      "episode: 435/500, time: 169, L_score: 25.153061224489864, R_score: 27.000508023996048, eL: 0.05, eR: 0.05\n",
      "episode: 436/500, time: 881, L_score: 256.18979591836336, R_score: 741.1384327204497, eL: 0.05, eR: 0.05\n",
      "episode: 437/500, time: 1130, L_score: 220.07755102040397, R_score: 349.75632911035217, eL: 0.05, eR: 0.05\n",
      "episode: 438/500, time: 241, L_score: 34.13673469387774, R_score: 51.02646697940518, eL: 0.05, eR: 0.05\n",
      "episode: 439/500, time: 500, L_score: 121.99599574671402, R_score: 299.67346938775825, eL: 0.05, eR: 0.05\n",
      "episode: 440/500, time: 805, L_score: 85.54489795918346, R_score: 541.6847467872905, eL: 0.05, eR: 0.05\n",
      "episode: 441/500, time: 78, L_score: 1.681737975054931, R_score: 14.126530612244887, eL: 0.05, eR: 0.05\n",
      "episode: 442/500, time: 1299, L_score: 41.2489795918335, R_score: 168.39597686953704, eL: 0.05, eR: 0.05\n",
      "episode: 443/500, time: 862, L_score: 277.7816326530618, R_score: 460.30934253126907, eL: 0.05, eR: 0.05\n",
      "episode: 444/500, time: 812, L_score: 209.77747162123342, R_score: 345.37142857143033, eL: 0.05, eR: 0.05\n",
      "episode: 445/500, time: 259, L_score: -106.60093888296474, R_score: -37.93061224489778, eL: 0.05, eR: 0.05\n",
      "episode: 446/500, time: 594, L_score: 76.04897959183667, R_score: 133.50447584774236, eL: 0.05, eR: 0.05\n",
      "episode: 447/500, time: 365, L_score: 54.74489795918387, R_score: 283.63144837316327, eL: 0.05, eR: 0.05\n",
      "episode: 448/500, time: 290, L_score: 40.44897959183675, R_score: 63.05426220995436, eL: 0.05, eR: 0.05\n",
      "episode: 449/500, time: 875, L_score: 333.0693877550995, R_score: 418.578141812826, eL: 0.05, eR: 0.05\n",
      "episode: 450/500, time: 588, L_score: 83.84897959183674, R_score: 498.5549917014955, eL: 0.05, eR: 0.05\n",
      "episode: 451/500, time: 653, L_score: 197.96122448979423, R_score: 305.4258559297398, eL: 0.05, eR: 0.05\n",
      "episode: 452/500, time: 134, L_score: 70.35714285714275, R_score: 39.992231287887606, eL: 0.05, eR: 0.05\n",
      "episode: 453/500, time: 2527, L_score: 680.2489795918633, R_score: 832.4820042148085, eL: 0.05, eR: 0.05\n",
      "episode: 454/500, time: 147, L_score: 13.056049917514638, R_score: 21.026530612244862, eL: 0.05, eR: 0.05\n",
      "episode: 455/500, time: 563, L_score: 37.75714285714272, R_score: 84.77527761717917, eL: 0.05, eR: 0.05\n",
      "episode: 456/500, time: 2454, L_score: 468.78210374217707, R_score: 510.5938775510303, eL: 0.05, eR: 0.05\n",
      "episode: 457/500, time: 90, L_score: 10.647254076423781, R_score: 18.591836734693864, eL: 0.05, eR: 0.05\n",
      "episode: 458/500, time: 296, L_score: 79.18343247270064, R_score: 79.58775510204059, eL: 0.05, eR: 0.05\n",
      "episode: 459/500, time: 717, L_score: 300.5612244897941, R_score: 150.32822352203883, eL: 0.05, eR: 0.05\n",
      "episode: 460/500, time: 540, L_score: 169.4646394916051, R_score: 338.7265306122482, eL: 0.05, eR: 0.05\n",
      "episode: 461/500, time: 89, L_score: 8.938824142739909, R_score: 18.695918367346923, eL: 0.05, eR: 0.05\n",
      "episode: 462/500, time: 418, L_score: 158.34081632652823, R_score: 304.2665319584251, eL: 0.05, eR: 0.05\n",
      "episode: 463/500, time: 426, L_score: 155.68059886796186, R_score: 309.93061224489935, eL: 0.05, eR: 0.05\n",
      "episode: 464/500, time: 3128, L_score: 520.5030246983721, R_score: 1179.7551020408218, eL: 0.05, eR: 0.05\n",
      "episode: 465/500, time: 422, L_score: 112.43265306122434, R_score: 99.09515996885187, eL: 0.05, eR: 0.05\n",
      "episode: 466/500, time: 318, L_score: 97.26530612244827, R_score: 61.024677682344304, eL: 0.05, eR: 0.05\n",
      "episode: 467/500, time: 531, L_score: 89.62857142857065, R_score: 361.1069316588206, eL: 0.05, eR: 0.05\n",
      "episode: 468/500, time: 1619, L_score: 157.4111498910425, R_score: 490.0142857142901, eL: 0.05, eR: 0.05\n",
      "episode: 469/500, time: 1837, L_score: 412.57747615094115, R_score: 459.4877551020435, eL: 0.05, eR: 0.05\n",
      "episode: 470/500, time: 208, L_score: 81.47346938775434, R_score: 50.61487446765043, eL: 0.05, eR: 0.05\n",
      "episode: 471/500, time: 474, L_score: 112.64081632652977, R_score: 81.22133703888514, eL: 0.05, eR: 0.05\n",
      "episode: 472/500, time: 51, L_score: 5.712244897959181, R_score: 15.561362952847707, eL: 0.05, eR: 0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 473/500, time: 430, L_score: 112.86530612244866, R_score: 91.63211622573674, eL: 0.05, eR: 0.05\n",
      "episode: 474/500, time: 1357, L_score: 266.348979591835, R_score: 844.770026135248, eL: 0.05, eR: 0.05\n",
      "episode: 475/500, time: 359, L_score: 96.32610149799905, R_score: 105.27142857142812, eL: 0.05, eR: 0.05\n",
      "episode: 476/500, time: 1615, L_score: 485.66938775510454, R_score: 819.2064636743434, eL: 0.05, eR: 0.05\n",
      "episode: 477/500, time: 377, L_score: 53.53673469387786, R_score: 248.86661742602473, eL: 0.05, eR: 0.05\n",
      "episode: 478/500, time: 217, L_score: 28.544897959183796, R_score: 39.04062721390271, eL: 0.05, eR: 0.05\n",
      "episode: 479/500, time: 995, L_score: 187.844897959182, R_score: 304.02966679273254, eL: 0.05, eR: 0.05\n",
      "episode: 480/500, time: 50, L_score: 5.408163265306121, R_score: 16.388981759554593, eL: 0.05, eR: 0.05\n",
      "episode: 481/500, time: 827, L_score: 163.46122448979304, R_score: 279.32057939805355, eL: 0.05, eR: 0.05\n",
      "episode: 482/500, time: 51, L_score: 6.120408163265304, R_score: 16.700906886688614, eL: 0.05, eR: 0.05\n",
      "episode: 483/500, time: 557, L_score: 78.94897959183571, R_score: 78.85381523264635, eL: 0.05, eR: 0.05\n",
      "episode: 484/500, time: 2491, L_score: 647.419598507082, R_score: 1356.0469387754858, eL: 0.05, eR: 0.05\n",
      "episode: 485/500, time: 166, L_score: 23.940816326530673, R_score: 6.526657067060803, eL: 0.05, eR: 0.05\n",
      "episode: 486/500, time: 951, L_score: 117.19270797569388, R_score: 218.2224489795902, eL: 0.05, eR: 0.05\n",
      "episode: 487/500, time: 202, L_score: 25.79631078199306, R_score: 42.538775510204246, eL: 0.05, eR: 0.05\n",
      "episode: 488/500, time: 2119, L_score: 553.7411872521748, R_score: 1180.671428571435, eL: 0.05, eR: 0.05\n",
      "episode: 489/500, time: 925, L_score: -39.276166703185396, R_score: 254.58979591836615, eL: 0.05, eR: 0.05\n",
      "episode: 490/500, time: 627, L_score: 139.32040816326472, R_score: 327.8819759695847, eL: 0.05, eR: 0.05\n",
      "episode: 491/500, time: 314, L_score: 37.214242402632934, R_score: 380.2183673469426, eL: 0.05, eR: 0.05\n",
      "episode: 492/500, time: 756, L_score: 157.83265306122226, R_score: 482.6272962406011, eL: 0.05, eR: 0.05\n",
      "episode: 493/500, time: 669, L_score: 146.3448979591838, R_score: 159.1204143457096, eL: 0.05, eR: 0.05\n",
      "episode: 494/500, time: 720, L_score: 249.82256815662112, R_score: 186.5224489795902, eL: 0.05, eR: 0.05\n",
      "episode: 495/500, time: 1645, L_score: 587.9693877551089, R_score: 535.125688187509, eL: 0.05, eR: 0.05\n",
      "episode: 496/500, time: 2447, L_score: 585.9530612244972, R_score: 814.810942474379, eL: 0.05, eR: 0.05\n",
      "episode: 497/500, time: 50, L_score: 5.916326530612243, R_score: 117.7794121431118, eL: 0.05, eR: 0.05\n",
      "episode: 498/500, time: 587, L_score: 60.87820366841501, R_score: 471.98775510204433, eL: 0.05, eR: 0.05\n",
      "episode: 499/500, time: 505, L_score: 70.96938775510228, R_score: 61.857335845801124, eL: 0.05, eR: 0.05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = Pong_Env()\n",
    "# state = [paddle_y, ball_x, ball_y, ball_speed, ball_xvel, ball_yvel]\n",
    "# state = [left_paddle_y, right_paddle_y, ball_x, ball_y, ball_speed, ball_xvel, ball_yvel]\n",
    "L_state_size = len(env.L_state_space)\n",
    "R_state_size = len(env.R_state_space)\n",
    "#print('state_size =', state_size)\n",
    "# action = [up, still, down]\n",
    "L_action_size = len(env.L_action_space)\n",
    "R_action_size = len(env.R_action_space)\n",
    "#print('action_size =', action_size)\n",
    "batch_size = 32 # Used for gradient descent\n",
    "n_episodes = 500 # Number of games played\n",
    "L_totalRewards = np.zeros(n_episodes)\n",
    "R_totalRewards = np.zeros(n_episodes)\n",
    "#rand_num = 0\n",
    "\n",
    "output_dir = 'model_output/pongSinglePaddleTraining'\n",
    "# output_dir = 'model_output/pongTraining'\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "L_agent = DQNLAgent(L_state_size, L_action_size, 0.05)\n",
    "\n",
    "L_agent.load('model_output\\pongSinglePaddleTrainingweightsL6_0100.hdf5')\n",
    "# L_agent.load('model_output\\pongSinglePaddleTrainingweightsL4_1500.hdf5')\n",
    "#L_agent.load('model_output\\pongSinglePaddleTrainingweightsL1_1000.hdf5')\n",
    "#L_agent.load('model_output\\pongtrainingweightsL8_7000.hdf5')\n",
    "R_agent = DQNRAgent(R_state_size, R_action_size, 0.05)\n",
    "#R_agent.load('model_output\\pongSinglePaddleTrainingweightsR5_1000.hdf5')\n",
    "R_agent.load('model_output\\pongSinglePaddleTrainingweightsR4_4000.hdf5')\n",
    "# R_agent.load('model_output\\pongSinglePaddleTrainingweightsR1_2000.hdf5')\n",
    "#done = False\n",
    "for e in range(n_episodes):\n",
    " #   print('e=', e)\n",
    "    L_state = env.reset()\n",
    "    L_state = np.reshape(L_state, (1, L_state_size))\n",
    "    R_state = env.reset()\n",
    "    R_state = np.reshape(R_state, (1, R_state_size))\n",
    "#    L_agent.memory = []\n",
    "#    R_agent.memory = []\n",
    "    #state = state[0]\n",
    "    done = False\n",
    "    for t in range(10000):\n",
    "       # if e > 0:\n",
    "           # env.Render(L_state[0], R_state[0])\n",
    " #           tau = .2\n",
    "  #      print(t) \n",
    " #       print('state_BeforeEpisode=', state)\n",
    "  #      print('nextState_Before=', next_state)\n",
    "        L_action = L_agent.act(L_state)\n",
    "      #  print('L_action = ', L_action, L_reward)\n",
    "        R_action = R_agent.act(R_state)\n",
    " #       print('R_action = ', R_action)\n",
    "        next_L_state, next_R_state, L_reward, R_reward, done, _ = env.step(L_action, R_action)\n",
    "#         next_L_state, L_reward, done, _ = env.step(L_action)\n",
    "#         next_R_state, R_reward, done, _ = env.step(R_action)\n",
    "  #      print('stateAfter =', state)\n",
    "  #      print('nextStateAfterStep =', next_state)\n",
    "     #   print('done =', done)\n",
    "     #   print('R_reward=', R_reward)\n",
    "     #   print([L_reward, R_reward, next_state[5]])\n",
    "     #   print(L_state[0])\n",
    "        #rounded_data = [round(elem,2) for elem in data]\n",
    "        #print(rounded_data)\n",
    "\n",
    "\n",
    "#         mystate = [round(x,2) for x in state]\n",
    "#         data = [mystate, reward, t, done]\n",
    "#         print(data)\n",
    "\n",
    "        next_L_state = np.reshape(next_L_state, [1, L_state_size])\n",
    "        next_R_state = np.reshape(next_R_state, [1, R_state_size])\n",
    "        \n",
    "        #if L_reward == -1 or L_reward == 0.1:\n",
    "        if L_reward == -1 or L_reward == 0.2 or L_reward == -.1 or L_reward == .1:    \n",
    "            if np.random.rand() <= .1:\n",
    "                L_agent.remember(L_state, L_action, L_reward, next_L_state, done)\n",
    "        else:\n",
    "            L_agent.remember(L_state, L_action, L_reward, next_L_state, done)\n",
    "        \n",
    "        \n",
    "        if R_reward == -2 or R_reward == 0.5 or R_reward == -.1 or R_reward == .1:    \n",
    "            if np.random.rand() <= .2:\n",
    "                R_agent.remember(R_state, R_action, R_reward, next_R_state, done)\n",
    "        else:\n",
    "            R_agent.remember(R_state, R_action, R_reward, next_R_state, done)\n",
    "        \n",
    "        \n",
    "   #     print(len(L_agent.memory))    \n",
    "            \n",
    "#         if R_reward == -1 or R_reward == 0.1:\n",
    "#             if np.random.rand() <= .1:\n",
    "#                 R_agent.remember(state, R_action, R_reward, next_state, done)\n",
    "#         else:\n",
    "#             R_agent.remember(state, R_action, R_reward, next_state, done)    \n",
    "   #     print(len(R_agent.memory))\n",
    " \n",
    "\n",
    "\n",
    " #       print('state[0][5] =', state[0][5])\n",
    "        #rand_num = random.randint(1,1001)\n",
    "  #      print('rand_num =', rand_num)\n",
    "  #      if state[0][5] < 0 and \\\n",
    "#         if state[0][2] < (WIDTH - 100) and \\\n",
    "#             L_reward >= 0 and \\\n",
    "#             L_reward <= 2.5 and \\\n",
    "#             random.random() > 0.99:\n",
    "                #print('L_r between 0 and 2 =', L_reward)\n",
    "  #      if L_reward != 0.1:\n",
    "  #              L_agent.remember(state, L_action, L_reward, next_state, done)\n",
    "                #print('activate1, >75 and == 1', rand_num, L_reward)\n",
    "#         if state[0][5] < 0 and \\\n",
    "#             L_reward != 1.0 and \\\n",
    "#             L_reward != 0:\n",
    "#                 L_agent.remember(state, L_action, L_reward, next_state, done)\n",
    "\n",
    "#         if state[0][2] < (WIDTH - 100) and \\\n",
    "#             L_reward < 0 or \\\n",
    "#             L_reward > 2.5:\n",
    "#                 #print('rand_num =', rand_num)\n",
    "#                 #print('L_r outside 0 and 2 =', L_reward)\n",
    "#                 L_agent.remember(state, L_action, L_reward, next_state, done)\n",
    "\n",
    "                #print('activate2')\n",
    "\n",
    "      #  if state[0][5] > 0 and \\\n",
    "#         if state[0][2] > 100 and \\\n",
    "#             R_reward >= 0 and \\\n",
    "#             R_reward <= 2.5 and \\\n",
    "#             random.random() > 0.99:\n",
    "  #      if R_reward != 0.1:\n",
    "  #              R_agent.remember(state, R_action, R_reward, next_state, done)\n",
    "                #print('activate3,  >75 and == 1', rand_num, R_reward)    \n",
    "#         if state[0][5] > 0 and \\\n",
    "#             R_reward != 1.0 and \\\n",
    "#             R_reward != 0:\n",
    "#                 R_agent.remember(state, R_action, R_reward, next_state, done)\n",
    "\n",
    "#         if state[0][2] < 100 and \\\n",
    "#             R_reward < 0 or \\\n",
    "#             R_reward > 2.5:\n",
    "#                 #print('rand_num =', rand_num)\n",
    "#                 #print('L_r outside 0 and 2 =', L_reward)\n",
    "#                 R_agent.remember(state, R_action, R_reward, next_state, done)\n",
    "                #print('activate4')\n",
    "        #print('memory =', len(L_agent.memory))\n",
    "        #if rand_num > 99:\n",
    "            #print('L_agent.memory =', L_agent.memory[0][2])\n",
    "            #print('shape of L_memory =', len(L_agent.memory))\n",
    "#         if rand_num < 1:\n",
    "#             print('R_agent.memory =', R_agent.memory[0])\n",
    "\n",
    "#        print('next_stateEnd =', next_state)\n",
    "        L_state = next_L_state\n",
    "        \n",
    "        R_state = next_R_state\n",
    "#        print('stateEnd after set to next_state =', state)\n",
    "\n",
    "    #    time.sleep(.01)\n",
    "#         if t % 150 == 0:\n",
    "#             if len(L_agent.memory) > batch_size:\n",
    "#                 L_agent.replay(batch_size)\n",
    "#                 L_agent.target_train()\n",
    "#                 #print('replay1')\n",
    "\n",
    "#             if len(R_agent.memory) > batch_size:\n",
    "#                 R_agent.replay(batch_size)\n",
    "#                 R_agent.target_train()\n",
    "#                 #print('replay2')\n",
    "    \n",
    "        if done:\n",
    "            #print(env.L_bounce_tally, env.R_bounce_tally)\n",
    "            print(\"episode: {}/{}, time: {}, L_score: {}, R_score: {}, eL: {:.2}, eR: {:.2}\".format(e, n_episodes, t, env.L_score, env.R_score, L_agent.epsilon, R_agent.epsilon))\n",
    "#            print(\"episode: {}/{}, time: {}, L_score: {}, LBounce_num: {}, eL: {:.2}\".format(e, n_episodes, t, env.L_score, env.L_bounce_tally, L_agent.epsilon))\n",
    "          #  print(\"episode: {}/{}, time: {}, R_score: {}, RBounce_num: {}, eR: {:.2}\".format(e, n_episodes, t, env.R_score, env.R_bounce_tally, R_agent.epsilon))\n",
    "            break\n",
    "\n",
    "    L_totalRewards[e] = env.L_score\n",
    "    R_totalRewards[e] = env.R_score\n",
    "                \n",
    "#     if len(L_agent.memory) > batch_size:\n",
    "#         L_agent.replay(batch_size)\n",
    "#         L_agent.target_train()\n",
    "        #print('replay1')\n",
    "\n",
    "#     if len(R_agent.memory) > batch_size:\n",
    "#         R_agent.replay(batch_size)\n",
    "#         if e % 2 == 0:\n",
    "#             R_agent.target_train()\n",
    "           # print('replay2')\n",
    "\n",
    "#     if e % 1000 == 0:\n",
    "#       #  L_agent.save(output_dir +  \"weightsL6_\" + '{:04d}'.format(e) + \".hdf5\")\n",
    "#         R_agent.save(output_dir +  \"weightsR7_\" + '{:04d}'.format(e) + \".hdf5\")\n",
    "        \n",
    "#plotRunningAverage(L_totalRewards)\n",
    "\n",
    "#plotRunningAverage(R_totalRewards)\n",
    "\n",
    "x = np.zeros(n_episodes)\n",
    "for i in range(0, n_episodes):\n",
    "    x[i] = i\n",
    "    \n",
    "y = L_totalRewards\n",
    "yMA = movingaverage(L_totalRewards, 50)\n",
    "\n",
    "z = R_totalRewards\n",
    "zMA = movingaverage(R_totalRewards, 50)\n",
    "# print(yMA)\n",
    "\n",
    "#plt.plot(x, y)\n",
    "\n",
    "plt.plot(x[len(x)-len(yMA):], yMA)\n",
    "plt.plot(x[len(x)-len(zMA):], zMA)\n",
    "plt.show()\n",
    "# plt.plot(R_totalRewards)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
